{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "536e356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8218918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b04789f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Open Ai\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d8589fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import JSONLoader\n",
    "import json\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c40a749f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\nitis\\\\Desktop\\\\Agentic_Ai_Learning\\\\Langraph'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4224adc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "# Step 1: Path to the Rag_data folder (adjust if needed)\n",
    "pdf_folder = './Rag_data'\n",
    "pdf_files = glob(os.path.join(pdf_folder, '*.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb0f7bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./Rag_data\\\\attention_paper.pdf',\n",
       " './Rag_data\\\\bert_paper.pdf',\n",
       " './Rag_data\\\\cnn_paper.pdf',\n",
       " './Rag_data\\\\resnet_paper_structured.pdf',\n",
       " './Rag_data\\\\transformer_paper.pdf',\n",
       " './Rag_data\\\\vision_transformer.pdf']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0122eb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded chunks: 46\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from glob import glob\n",
    "\n",
    "def create_simple_chunks(file_path, chunk_size=3500, chunk_overlap=200):\n",
    "    loader = PyMuPDFLoader(file_path)\n",
    "    doc_pages = loader.load()\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return splitter.split_documents(doc_pages)\n",
    "\n",
    "# ðŸ›  Corrected folder path!\n",
    "pdf_files = glob('./Rag_data/*.pdf')\n",
    "\n",
    "paper_docs = []\n",
    "for fp in pdf_files:\n",
    "    paper_docs.extend(create_simple_chunks(file_path=fp))\n",
    "\n",
    "print(f\"âœ… Loaded chunks: {len(paper_docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3097f43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:22:47+00:00', 'source': './Rag_data\\\\attention_paper.pdf', 'file_path': './Rag_data\\\\attention_paper.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:22:47+00:00', 'trapped': '', 'modDate': \"D:20250522022247+00'00'\", 'creationDate': \"D:20250522022247+00'00'\", 'page': 0}, page_content='The attention mechanism allows neural networks to dynamically focus on different parts of the input\\nsequence.\\nIt was first introduced in the context of machine translation with the goal of improving alignment\\nbetween source and target languages.\\nAttention mechanisms assign varying levels of importance to different words or tokens in a sequence,\\nimproving context comprehension.\\nIn practice, attention uses query, key, and value matrices to calculate weighted outputs.\\nSelf-attention, introduced in the Transformer model, enables each token to attend to all other tokens in\\nthe sequence.\\nThis allows the model to capture relationships regardless of distance in the input.\\nMulti-head attention enables the model to jointly attend to information from different representation\\nsubspaces.\\nThe attention mechanism is a core part of architectures like BERT, GPT, and T5.\\nIt supports language understanding, generation, translation, and more.\\nThe attention mechanism allows neural networks to dynamically focus on different parts of the input\\nsequence.\\nIt was first introduced in the context of machine translation with the goal of improving alignment\\nbetween source and target languages.\\nAttention mechanisms assign varying levels of importance to different words or tokens in a sequence,\\nimproving context comprehension.\\nIn practice, attention uses query, key, and value matrices to calculate weighted outputs.\\nSelf-attention, introduced in the Transformer model, enables each token to attend to all other tokens in\\nthe sequence.\\nThis allows the model to capture relationships regardless of distance in the input.\\nMulti-head attention enables the model to jointly attend to information from different representation\\nsubspaces.\\nThe attention mechanism is a core part of architectures like BERT, GPT, and T5.\\nIt supports language understanding, generation, translation, and more.\\nThe attention mechanism allows neural networks to dynamically focus on different parts of the input\\nsequence.\\nIt was first introduced in the context of machine translation with the goal of improving alignment\\nbetween source and target languages.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:22:47+00:00', 'source': './Rag_data\\\\attention_paper.pdf', 'file_path': './Rag_data\\\\attention_paper.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:22:47+00:00', 'trapped': '', 'modDate': \"D:20250522022247+00'00'\", 'creationDate': \"D:20250522022247+00'00'\", 'page': 1}, page_content='Attention mechanisms assign varying levels of importance to different words or tokens in a sequence,\\nimproving context comprehension.\\nIn practice, attention uses query, key, and value matrices to calculate weighted outputs.\\nSelf-attention, introduced in the Transformer model, enables each token to attend to all other tokens in\\nthe sequence.\\nThis allows the model to capture relationships regardless of distance in the input.\\nMulti-head attention enables the model to jointly attend to information from different representation\\nsubspaces.\\nThe attention mechanism is a core part of architectures like BERT, GPT, and T5.\\nIt supports language understanding, generation, translation, and more.\\nThe attention mechanism allows neural networks to dynamically focus on different parts of the input\\nsequence.\\nIt was first introduced in the context of machine translation with the goal of improving alignment\\nbetween source and target languages.\\nAttention mechanisms assign varying levels of importance to different words or tokens in a sequence,\\nimproving context comprehension.\\nIn practice, attention uses query, key, and value matrices to calculate weighted outputs.\\nSelf-attention, introduced in the Transformer model, enables each token to attend to all other tokens in\\nthe sequence.\\nThis allows the model to capture relationships regardless of distance in the input.\\nMulti-head attention enables the model to jointly attend to information from different representation\\nsubspaces.\\nThe attention mechanism is a core part of architectures like BERT, GPT, and T5.\\nIt supports language understanding, generation, translation, and more.\\nThe attention mechanism allows neural networks to dynamically focus on different parts of the input\\nsequence.\\nIt was first introduced in the context of machine translation with the goal of improving alignment\\nbetween source and target languages.\\nAttention mechanisms assign varying levels of importance to different words or tokens in a sequence,\\nimproving context comprehension.\\nIn practice, attention uses query, key, and value matrices to calculate weighted outputs.\\nSelf-attention, introduced in the Transformer model, enables each token to attend to all other tokens in\\nthe sequence.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:22:47+00:00', 'source': './Rag_data\\\\attention_paper.pdf', 'file_path': './Rag_data\\\\attention_paper.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:22:47+00:00', 'trapped': '', 'modDate': \"D:20250522022247+00'00'\", 'creationDate': \"D:20250522022247+00'00'\", 'page': 2}, page_content='This allows the model to capture relationships regardless of distance in the input.\\nMulti-head attention enables the model to jointly attend to information from different representation\\nsubspaces.\\nThe attention mechanism is a core part of architectures like BERT, GPT, and T5.\\nIt supports language understanding, generation, translation, and more.\\nThe attention mechanism allows neural networks to dynamically focus on different parts of the input\\nsequence.\\nIt was first introduced in the context of machine translation with the goal of improving alignment\\nbetween source and target languages.\\nAttention mechanisms assign varying levels of importance to different words or tokens in a sequence,\\nimproving context comprehension.\\nIn practice, attention uses query, key, and value matrices to calculate weighted outputs.\\nSelf-attention, introduced in the Transformer model, enables each token to attend to all other tokens in\\nthe sequence.\\nThis allows the model to capture relationships regardless of distance in the input.\\nMulti-head attention enables the model to jointly attend to information from different representation\\nsubspaces.\\nThe attention mechanism is a core part of architectures like BERT, GPT, and T5.\\nIt supports language understanding, generation, translation, and more.\\nThe attention mechanism allows neural networks to dynamically focus on different parts of the input\\nsequence.\\nIt was first introduced in the context of machine translation with the goal of improving alignment\\nbetween source and target languages.\\nAttention mechanisms assign varying levels of importance to different words or tokens in a sequence,\\nimproving context comprehension.\\nIn practice, attention uses query, key, and value matrices to calculate weighted outputs.\\nSelf-attention, introduced in the Transformer model, enables each token to attend to all other tokens in\\nthe sequence.\\nThis allows the model to capture relationships regardless of distance in the input.\\nMulti-head attention enables the model to jointly attend to information from different representation\\nsubspaces.\\nThe attention mechanism is a core part of architectures like BERT, GPT, and T5.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:22:47+00:00', 'source': './Rag_data\\\\attention_paper.pdf', 'file_path': './Rag_data\\\\attention_paper.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:22:47+00:00', 'trapped': '', 'modDate': \"D:20250522022247+00'00'\", 'creationDate': \"D:20250522022247+00'00'\", 'page': 3}, page_content='It supports language understanding, generation, translation, and more.\\nThe attention mechanism allows neural networks to dynamically focus on different parts of the input\\nsequence.\\nIt was first introduced in the context of machine translation with the goal of improving alignment\\nbetween source and target languages.\\nAttention mechanisms assign varying levels of importance to different words or tokens in a sequence,\\nimproving context comprehension.\\nIn practice, attention uses query, key, and value matrices to calculate weighted outputs.\\nSelf-attention, introduced in the Transformer model, enables each token to attend to all other tokens in\\nthe sequence.\\nThis allows the model to capture relationships regardless of distance in the input.\\nMulti-head attention enables the model to jointly attend to information from different representation\\nsubspaces.\\nThe attention mechanism is a core part of architectures like BERT, GPT, and T5.\\nIt supports language understanding, generation, translation, and more.\\nThe attention mechanism allows neural networks to dynamically focus on different parts of the input\\nsequence.\\nIt was first introduced in the context of machine translation with the goal of improving alignment\\nbetween source and target languages.\\nAttention mechanisms assign varying levels of importance to different words or tokens in a sequence,\\nimproving context comprehension.\\nIn practice, attention uses query, key, and value matrices to calculate weighted outputs.\\nSelf-attention, introduced in the Transformer model, enables each token to attend to all other tokens in\\nthe sequence.\\nThis allows the model to capture relationships regardless of distance in the input.\\nMulti-head attention enables the model to jointly attend to information from different representation\\nsubspaces.\\nThe attention mechanism is a core part of architectures like BERT, GPT, and T5.\\nIt supports language understanding, generation, translation, and more.\\nThe attention mechanism allows neural networks to dynamically focus on different parts of the input\\nsequence.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:22:47+00:00', 'source': './Rag_data\\\\attention_paper.pdf', 'file_path': './Rag_data\\\\attention_paper.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:22:47+00:00', 'trapped': '', 'modDate': \"D:20250522022247+00'00'\", 'creationDate': \"D:20250522022247+00'00'\", 'page': 4}, page_content='It was first introduced in the context of machine translation with the goal of improving alignment\\nbetween source and target languages.\\nAttention mechanisms assign varying levels of importance to different words or tokens in a sequence,\\nimproving context comprehension.\\nIn practice, attention uses query, key, and value matrices to calculate weighted outputs.\\nSelf-attention, introduced in the Transformer model, enables each token to attend to all other tokens in\\nthe sequence.\\nThis allows the model to capture relationships regardless of distance in the input.\\nMulti-head attention enables the model to jointly attend to information from different representation\\nsubspaces.\\nThe attention mechanism is a core part of architectures like BERT, GPT, and T5.\\nIt supports language understanding, generation, translation, and more.\\nThe attention mechanism allows neural networks to dynamically focus on different parts of the input\\nsequence.\\nIt was first introduced in the context of machine translation with the goal of improving alignment\\nbetween source and target languages.\\nAttention mechanisms assign varying levels of importance to different words or tokens in a sequence,\\nimproving context comprehension.\\nIn practice, attention uses query, key, and value matrices to calculate weighted outputs.\\nSelf-attention, introduced in the Transformer model, enables each token to attend to all other tokens in\\nthe sequence.\\nThis allows the model to capture relationships regardless of distance in the input.\\nMulti-head attention enables the model to jointly attend to information from different representation\\nsubspaces.\\nThe attention mechanism is a core part of architectures like BERT, GPT, and T5.\\nIt supports language understanding, generation, translation, and more.\\nThe attention mechanism allows neural networks to dynamically focus on different parts of the input\\nsequence.\\nIt was first introduced in the context of machine translation with the goal of improving alignment\\nbetween source and target languages.\\nAttention mechanisms assign varying levels of importance to different words or tokens in a sequence,\\nimproving context comprehension.\\nIn practice, attention uses query, key, and value matrices to calculate weighted outputs.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:22:47+00:00', 'source': './Rag_data\\\\attention_paper.pdf', 'file_path': './Rag_data\\\\attention_paper.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:22:47+00:00', 'trapped': '', 'modDate': \"D:20250522022247+00'00'\", 'creationDate': \"D:20250522022247+00'00'\", 'page': 5}, page_content='Self-attention, introduced in the Transformer model, enables each token to attend to all other tokens in\\nthe sequence.\\nThis allows the model to capture relationships regardless of distance in the input.\\nMulti-head attention enables the model to jointly attend to information from different representation\\nsubspaces.\\nThe attention mechanism is a core part of architectures like BERT, GPT, and T5.\\nIt supports language understanding, generation, translation, and more.\\nThe attention mechanism allows neural networks to dynamically focus on different parts of the input\\nsequence.\\nIt was first introduced in the context of machine translation with the goal of improving alignment\\nbetween source and target languages.\\nAttention mechanisms assign varying levels of importance to different words or tokens in a sequence,\\nimproving context comprehension.\\nIn practice, attention uses query, key, and value matrices to calculate weighted outputs.\\nSelf-attention, introduced in the Transformer model, enables each token to attend to all other tokens in\\nthe sequence.\\nThis allows the model to capture relationships regardless of distance in the input.\\nMulti-head attention enables the model to jointly attend to information from different representation\\nsubspaces.\\nThe attention mechanism is a core part of architectures like BERT, GPT, and T5.\\nIt supports language understanding, generation, translation, and more.\\nThe attention mechanism allows neural networks to dynamically focus on different parts of the input\\nsequence.\\nIt was first introduced in the context of machine translation with the goal of improving alignment\\nbetween source and target languages.\\nAttention mechanisms assign varying levels of importance to different words or tokens in a sequence,\\nimproving context comprehension.\\nIn practice, attention uses query, key, and value matrices to calculate weighted outputs.\\nSelf-attention, introduced in the Transformer model, enables each token to attend to all other tokens in\\nthe sequence.\\nThis allows the model to capture relationships regardless of distance in the input.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:22:47+00:00', 'source': './Rag_data\\\\attention_paper.pdf', 'file_path': './Rag_data\\\\attention_paper.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:22:47+00:00', 'trapped': '', 'modDate': \"D:20250522022247+00'00'\", 'creationDate': \"D:20250522022247+00'00'\", 'page': 6}, page_content='Multi-head attention enables the model to jointly attend to information from different representation\\nsubspaces.\\nThe attention mechanism is a core part of architectures like BERT, GPT, and T5.\\nIt supports language understanding, generation, translation, and more.\\nThe attention mechanism allows neural networks to dynamically focus on different parts of the input\\nsequence.\\nIt was first introduced in the context of machine translation with the goal of improving alignment\\nbetween source and target languages.\\nAttention mechanisms assign varying levels of importance to different words or tokens in a sequence,\\nimproving context comprehension.\\nIn practice, attention uses query, key, and value matrices to calculate weighted outputs.\\nSelf-attention, introduced in the Transformer model, enables each token to attend to all other tokens in\\nthe sequence.\\nThis allows the model to capture relationships regardless of distance in the input.\\nMulti-head attention enables the model to jointly attend to information from different representation\\nsubspaces.\\nThe attention mechanism is a core part of architectures like BERT, GPT, and T5.\\nIt supports language understanding, generation, translation, and more.\\nThe attention mechanism allows neural networks to dynamically focus on different parts of the input\\nsequence.\\nIt was first introduced in the context of machine translation with the goal of improving alignment\\nbetween source and target languages.\\nAttention mechanisms assign varying levels of importance to different words or tokens in a sequence,\\nimproving context comprehension.\\nIn practice, attention uses query, key, and value matrices to calculate weighted outputs.\\nSelf-attention, introduced in the Transformer model, enables each token to attend to all other tokens in\\nthe sequence.\\nThis allows the model to capture relationships regardless of distance in the input.\\nMulti-head attention enables the model to jointly attend to information from different representation\\nsubspaces.\\nThe attention mechanism is a core part of architectures like BERT, GPT, and T5.\\nIt supports language understanding, generation, translation, and more.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:22:47+00:00', 'source': './Rag_data\\\\attention_paper.pdf', 'file_path': './Rag_data\\\\attention_paper.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:22:47+00:00', 'trapped': '', 'modDate': \"D:20250522022247+00'00'\", 'creationDate': \"D:20250522022247+00'00'\", 'page': 7}, page_content='The attention mechanism allows neural networks to dynamically focus on different parts of the input\\nsequence.\\nIt was first introduced in the context of machine translation with the goal of improving alignment\\nbetween source and target languages.\\nAttention mechanisms assign varying levels of importance to different words or tokens in a sequence,\\nimproving context comprehension.\\nIn practice, attention uses query, key, and value matrices to calculate weighted outputs.\\nSelf-attention, introduced in the Transformer model, enables each token to attend to all other tokens in\\nthe sequence.\\nThis allows the model to capture relationships regardless of distance in the input.\\nMulti-head attention enables the model to jointly attend to information from different representation\\nsubspaces.\\nThe attention mechanism is a core part of architectures like BERT, GPT, and T5.\\nIt supports language understanding, generation, translation, and more.\\nThe attention mechanism allows neural networks to dynamically focus on different parts of the input\\nsequence.\\nIt was first introduced in the context of machine translation with the goal of improving alignment\\nbetween source and target languages.\\nAttention mechanisms assign varying levels of importance to different words or tokens in a sequence,\\nimproving context comprehension.\\nIn practice, attention uses query, key, and value matrices to calculate weighted outputs.\\nSelf-attention, introduced in the Transformer model, enables each token to attend to all other tokens in\\nthe sequence.\\nThis allows the model to capture relationships regardless of distance in the input.\\nMulti-head attention enables the model to jointly attend to information from different representation\\nsubspaces.\\nThe attention mechanism is a core part of architectures like BERT, GPT, and T5.\\nIt supports language understanding, generation, translation, and more.\\nThe attention mechanism allows neural networks to dynamically focus on different parts of the input\\nsequence.\\nIt was first introduced in the context of machine translation with the goal of improving alignment\\nbetween source and target languages.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:22:47+00:00', 'source': './Rag_data\\\\attention_paper.pdf', 'file_path': './Rag_data\\\\attention_paper.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:22:47+00:00', 'trapped': '', 'modDate': \"D:20250522022247+00'00'\", 'creationDate': \"D:20250522022247+00'00'\", 'page': 8}, page_content='Attention mechanisms assign varying levels of importance to different words or tokens in a sequence,\\nimproving context comprehension.\\nIn practice, attention uses query, key, and value matrices to calculate weighted outputs.\\nSelf-attention, introduced in the Transformer model, enables each token to attend to all other tokens in\\nthe sequence.\\nThis allows the model to capture relationships regardless of distance in the input.\\nMulti-head attention enables the model to jointly attend to information from different representation\\nsubspaces.\\nThe attention mechanism is a core part of architectures like BERT, GPT, and T5.\\nIt supports language understanding, generation, translation, and more.\\nThe attention mechanism allows neural networks to dynamically focus on different parts of the input\\nsequence.\\nIt was first introduced in the context of machine translation with the goal of improving alignment\\nbetween source and target languages.\\nAttention mechanisms assign varying levels of importance to different words or tokens in a sequence,\\nimproving context comprehension.\\nIn practice, attention uses query, key, and value matrices to calculate weighted outputs.\\nSelf-attention, introduced in the Transformer model, enables each token to attend to all other tokens in\\nthe sequence.\\nThis allows the model to capture relationships regardless of distance in the input.\\nMulti-head attention enables the model to jointly attend to information from different representation\\nsubspaces.\\nThe attention mechanism is a core part of architectures like BERT, GPT, and T5.\\nIt supports language understanding, generation, translation, and more.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:22:47+00:00', 'source': './Rag_data\\\\bert_paper.pdf', 'file_path': './Rag_data\\\\bert_paper.pdf', 'total_pages': 7, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:22:47+00:00', 'trapped': '', 'modDate': \"D:20250522022247+00'00'\", 'creationDate': \"D:20250522022247+00'00'\", 'page': 0}, page_content='BERT (Bidirectional Encoder Representations from Transformers) is a Transformer-based model\\npre-trained on large text corpora.\\nIt introduced two pre-training tasks: Masked Language Modeling (MLM) and Next Sentence Prediction\\n(NSP).\\nMLM involves randomly masking tokens in the input and training the model to predict them, enabling\\nbidirectional context capture.\\nNSP trains the model to understand sentence relationships.\\nBERT is fine-tuned for specific downstream tasks like classification, question answering, and named\\nentity recognition.\\nVariants include RoBERTa, ALBERT, and DistilBERT.\\nBERT achieved state-of-the-art results on multiple NLP benchmarks such as GLUE and SQuAD.\\nBERT (Bidirectional Encoder Representations from Transformers) is a Transformer-based model\\npre-trained on large text corpora.\\nIt introduced two pre-training tasks: Masked Language Modeling (MLM) and Next Sentence Prediction\\n(NSP).\\nMLM involves randomly masking tokens in the input and training the model to predict them, enabling\\nbidirectional context capture.\\nNSP trains the model to understand sentence relationships.\\nBERT is fine-tuned for specific downstream tasks like classification, question answering, and named\\nentity recognition.\\nVariants include RoBERTa, ALBERT, and DistilBERT.\\nBERT achieved state-of-the-art results on multiple NLP benchmarks such as GLUE and SQuAD.\\nBERT (Bidirectional Encoder Representations from Transformers) is a Transformer-based model\\npre-trained on large text corpora.\\nIt introduced two pre-training tasks: Masked Language Modeling (MLM) and Next Sentence Prediction\\n(NSP).\\nMLM involves randomly masking tokens in the input and training the model to predict them, enabling\\nbidirectional context capture.\\nNSP trains the model to understand sentence relationships.\\nBERT is fine-tuned for specific downstream tasks like classification, question answering, and named\\nentity recognition.\\nVariants include RoBERTa, ALBERT, and DistilBERT.\\nBERT achieved state-of-the-art results on multiple NLP benchmarks such as GLUE and SQuAD.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:22:47+00:00', 'source': './Rag_data\\\\bert_paper.pdf', 'file_path': './Rag_data\\\\bert_paper.pdf', 'total_pages': 7, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:22:47+00:00', 'trapped': '', 'modDate': \"D:20250522022247+00'00'\", 'creationDate': \"D:20250522022247+00'00'\", 'page': 1}, page_content='BERT (Bidirectional Encoder Representations from Transformers) is a Transformer-based model\\npre-trained on large text corpora.\\nIt introduced two pre-training tasks: Masked Language Modeling (MLM) and Next Sentence Prediction\\n(NSP).\\nMLM involves randomly masking tokens in the input and training the model to predict them, enabling\\nbidirectional context capture.\\nNSP trains the model to understand sentence relationships.\\nBERT is fine-tuned for specific downstream tasks like classification, question answering, and named\\nentity recognition.\\nVariants include RoBERTa, ALBERT, and DistilBERT.\\nBERT achieved state-of-the-art results on multiple NLP benchmarks such as GLUE and SQuAD.\\nBERT (Bidirectional Encoder Representations from Transformers) is a Transformer-based model\\npre-trained on large text corpora.\\nIt introduced two pre-training tasks: Masked Language Modeling (MLM) and Next Sentence Prediction\\n(NSP).\\nMLM involves randomly masking tokens in the input and training the model to predict them, enabling\\nbidirectional context capture.\\nNSP trains the model to understand sentence relationships.\\nBERT is fine-tuned for specific downstream tasks like classification, question answering, and named\\nentity recognition.\\nVariants include RoBERTa, ALBERT, and DistilBERT.\\nBERT achieved state-of-the-art results on multiple NLP benchmarks such as GLUE and SQuAD.\\nBERT (Bidirectional Encoder Representations from Transformers) is a Transformer-based model\\npre-trained on large text corpora.\\nIt introduced two pre-training tasks: Masked Language Modeling (MLM) and Next Sentence Prediction\\n(NSP).\\nMLM involves randomly masking tokens in the input and training the model to predict them, enabling\\nbidirectional context capture.\\nNSP trains the model to understand sentence relationships.\\nBERT is fine-tuned for specific downstream tasks like classification, question answering, and named\\nentity recognition.\\nVariants include RoBERTa, ALBERT, and DistilBERT.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:22:47+00:00', 'source': './Rag_data\\\\bert_paper.pdf', 'file_path': './Rag_data\\\\bert_paper.pdf', 'total_pages': 7, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:22:47+00:00', 'trapped': '', 'modDate': \"D:20250522022247+00'00'\", 'creationDate': \"D:20250522022247+00'00'\", 'page': 2}, page_content='BERT achieved state-of-the-art results on multiple NLP benchmarks such as GLUE and SQuAD.\\nBERT (Bidirectional Encoder Representations from Transformers) is a Transformer-based model\\npre-trained on large text corpora.\\nIt introduced two pre-training tasks: Masked Language Modeling (MLM) and Next Sentence Prediction\\n(NSP).\\nMLM involves randomly masking tokens in the input and training the model to predict them, enabling\\nbidirectional context capture.\\nNSP trains the model to understand sentence relationships.\\nBERT is fine-tuned for specific downstream tasks like classification, question answering, and named\\nentity recognition.\\nVariants include RoBERTa, ALBERT, and DistilBERT.\\nBERT achieved state-of-the-art results on multiple NLP benchmarks such as GLUE and SQuAD.\\nBERT (Bidirectional Encoder Representations from Transformers) is a Transformer-based model\\npre-trained on large text corpora.\\nIt introduced two pre-training tasks: Masked Language Modeling (MLM) and Next Sentence Prediction\\n(NSP).\\nMLM involves randomly masking tokens in the input and training the model to predict them, enabling\\nbidirectional context capture.\\nNSP trains the model to understand sentence relationships.\\nBERT is fine-tuned for specific downstream tasks like classification, question answering, and named\\nentity recognition.\\nVariants include RoBERTa, ALBERT, and DistilBERT.\\nBERT achieved state-of-the-art results on multiple NLP benchmarks such as GLUE and SQuAD.\\nBERT (Bidirectional Encoder Representations from Transformers) is a Transformer-based model\\npre-trained on large text corpora.\\nIt introduced two pre-training tasks: Masked Language Modeling (MLM) and Next Sentence Prediction\\n(NSP).\\nMLM involves randomly masking tokens in the input and training the model to predict them, enabling\\nbidirectional context capture.\\nNSP trains the model to understand sentence relationships.\\nBERT is fine-tuned for specific downstream tasks like classification, question answering, and named\\nentity recognition.\\nVariants include RoBERTa, ALBERT, and DistilBERT.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:22:47+00:00', 'source': './Rag_data\\\\bert_paper.pdf', 'file_path': './Rag_data\\\\bert_paper.pdf', 'total_pages': 7, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:22:47+00:00', 'trapped': '', 'modDate': \"D:20250522022247+00'00'\", 'creationDate': \"D:20250522022247+00'00'\", 'page': 3}, page_content='BERT achieved state-of-the-art results on multiple NLP benchmarks such as GLUE and SQuAD.\\nBERT (Bidirectional Encoder Representations from Transformers) is a Transformer-based model\\npre-trained on large text corpora.\\nIt introduced two pre-training tasks: Masked Language Modeling (MLM) and Next Sentence Prediction\\n(NSP).\\nMLM involves randomly masking tokens in the input and training the model to predict them, enabling\\nbidirectional context capture.\\nNSP trains the model to understand sentence relationships.\\nBERT is fine-tuned for specific downstream tasks like classification, question answering, and named\\nentity recognition.\\nVariants include RoBERTa, ALBERT, and DistilBERT.\\nBERT achieved state-of-the-art results on multiple NLP benchmarks such as GLUE and SQuAD.\\nBERT (Bidirectional Encoder Representations from Transformers) is a Transformer-based model\\npre-trained on large text corpora.\\nIt introduced two pre-training tasks: Masked Language Modeling (MLM) and Next Sentence Prediction\\n(NSP).\\nMLM involves randomly masking tokens in the input and training the model to predict them, enabling\\nbidirectional context capture.\\nNSP trains the model to understand sentence relationships.\\nBERT is fine-tuned for specific downstream tasks like classification, question answering, and named\\nentity recognition.\\nVariants include RoBERTa, ALBERT, and DistilBERT.\\nBERT achieved state-of-the-art results on multiple NLP benchmarks such as GLUE and SQuAD.\\nBERT (Bidirectional Encoder Representations from Transformers) is a Transformer-based model\\npre-trained on large text corpora.\\nIt introduced two pre-training tasks: Masked Language Modeling (MLM) and Next Sentence Prediction\\n(NSP).\\nMLM involves randomly masking tokens in the input and training the model to predict them, enabling\\nbidirectional context capture.\\nNSP trains the model to understand sentence relationships.\\nBERT is fine-tuned for specific downstream tasks like classification, question answering, and named\\nentity recognition.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:22:47+00:00', 'source': './Rag_data\\\\bert_paper.pdf', 'file_path': './Rag_data\\\\bert_paper.pdf', 'total_pages': 7, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:22:47+00:00', 'trapped': '', 'modDate': \"D:20250522022247+00'00'\", 'creationDate': \"D:20250522022247+00'00'\", 'page': 4}, page_content='Variants include RoBERTa, ALBERT, and DistilBERT.\\nBERT achieved state-of-the-art results on multiple NLP benchmarks such as GLUE and SQuAD.\\nBERT (Bidirectional Encoder Representations from Transformers) is a Transformer-based model\\npre-trained on large text corpora.\\nIt introduced two pre-training tasks: Masked Language Modeling (MLM) and Next Sentence Prediction\\n(NSP).\\nMLM involves randomly masking tokens in the input and training the model to predict them, enabling\\nbidirectional context capture.\\nNSP trains the model to understand sentence relationships.\\nBERT is fine-tuned for specific downstream tasks like classification, question answering, and named\\nentity recognition.\\nVariants include RoBERTa, ALBERT, and DistilBERT.\\nBERT achieved state-of-the-art results on multiple NLP benchmarks such as GLUE and SQuAD.\\nBERT (Bidirectional Encoder Representations from Transformers) is a Transformer-based model\\npre-trained on large text corpora.\\nIt introduced two pre-training tasks: Masked Language Modeling (MLM) and Next Sentence Prediction\\n(NSP).\\nMLM involves randomly masking tokens in the input and training the model to predict them, enabling\\nbidirectional context capture.\\nNSP trains the model to understand sentence relationships.\\nBERT is fine-tuned for specific downstream tasks like classification, question answering, and named\\nentity recognition.\\nVariants include RoBERTa, ALBERT, and DistilBERT.\\nBERT achieved state-of-the-art results on multiple NLP benchmarks such as GLUE and SQuAD.\\nBERT (Bidirectional Encoder Representations from Transformers) is a Transformer-based model\\npre-trained on large text corpora.\\nIt introduced two pre-training tasks: Masked Language Modeling (MLM) and Next Sentence Prediction\\n(NSP).\\nMLM involves randomly masking tokens in the input and training the model to predict them, enabling\\nbidirectional context capture.\\nNSP trains the model to understand sentence relationships.\\nBERT is fine-tuned for specific downstream tasks like classification, question answering, and named\\nentity recognition.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:22:47+00:00', 'source': './Rag_data\\\\bert_paper.pdf', 'file_path': './Rag_data\\\\bert_paper.pdf', 'total_pages': 7, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:22:47+00:00', 'trapped': '', 'modDate': \"D:20250522022247+00'00'\", 'creationDate': \"D:20250522022247+00'00'\", 'page': 5}, page_content='Variants include RoBERTa, ALBERT, and DistilBERT.\\nBERT achieved state-of-the-art results on multiple NLP benchmarks such as GLUE and SQuAD.\\nBERT (Bidirectional Encoder Representations from Transformers) is a Transformer-based model\\npre-trained on large text corpora.\\nIt introduced two pre-training tasks: Masked Language Modeling (MLM) and Next Sentence Prediction\\n(NSP).\\nMLM involves randomly masking tokens in the input and training the model to predict them, enabling\\nbidirectional context capture.\\nNSP trains the model to understand sentence relationships.\\nBERT is fine-tuned for specific downstream tasks like classification, question answering, and named\\nentity recognition.\\nVariants include RoBERTa, ALBERT, and DistilBERT.\\nBERT achieved state-of-the-art results on multiple NLP benchmarks such as GLUE and SQuAD.\\nBERT (Bidirectional Encoder Representations from Transformers) is a Transformer-based model\\npre-trained on large text corpora.\\nIt introduced two pre-training tasks: Masked Language Modeling (MLM) and Next Sentence Prediction\\n(NSP).\\nMLM involves randomly masking tokens in the input and training the model to predict them, enabling\\nbidirectional context capture.\\nNSP trains the model to understand sentence relationships.\\nBERT is fine-tuned for specific downstream tasks like classification, question answering, and named\\nentity recognition.\\nVariants include RoBERTa, ALBERT, and DistilBERT.\\nBERT achieved state-of-the-art results on multiple NLP benchmarks such as GLUE and SQuAD.\\nBERT (Bidirectional Encoder Representations from Transformers) is a Transformer-based model\\npre-trained on large text corpora.\\nIt introduced two pre-training tasks: Masked Language Modeling (MLM) and Next Sentence Prediction\\n(NSP).\\nMLM involves randomly masking tokens in the input and training the model to predict them, enabling\\nbidirectional context capture.\\nNSP trains the model to understand sentence relationships.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:22:47+00:00', 'source': './Rag_data\\\\bert_paper.pdf', 'file_path': './Rag_data\\\\bert_paper.pdf', 'total_pages': 7, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:22:47+00:00', 'trapped': '', 'modDate': \"D:20250522022247+00'00'\", 'creationDate': \"D:20250522022247+00'00'\", 'page': 6}, page_content='BERT is fine-tuned for specific downstream tasks like classification, question answering, and named\\nentity recognition.\\nVariants include RoBERTa, ALBERT, and DistilBERT.\\nBERT achieved state-of-the-art results on multiple NLP benchmarks such as GLUE and SQuAD.\\nBERT (Bidirectional Encoder Representations from Transformers) is a Transformer-based model\\npre-trained on large text corpora.\\nIt introduced two pre-training tasks: Masked Language Modeling (MLM) and Next Sentence Prediction\\n(NSP).\\nMLM involves randomly masking tokens in the input and training the model to predict them, enabling\\nbidirectional context capture.\\nNSP trains the model to understand sentence relationships.\\nBERT is fine-tuned for specific downstream tasks like classification, question answering, and named\\nentity recognition.\\nVariants include RoBERTa, ALBERT, and DistilBERT.\\nBERT achieved state-of-the-art results on multiple NLP benchmarks such as GLUE and SQuAD.\\nBERT (Bidirectional Encoder Representations from Transformers) is a Transformer-based model\\npre-trained on large text corpora.\\nIt introduced two pre-training tasks: Masked Language Modeling (MLM) and Next Sentence Prediction\\n(NSP).\\nMLM involves randomly masking tokens in the input and training the model to predict them, enabling\\nbidirectional context capture.\\nNSP trains the model to understand sentence relationships.\\nBERT is fine-tuned for specific downstream tasks like classification, question answering, and named\\nentity recognition.\\nVariants include RoBERTa, ALBERT, and DistilBERT.\\nBERT achieved state-of-the-art results on multiple NLP benchmarks such as GLUE and SQuAD.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:22:47+00:00', 'source': './Rag_data\\\\cnn_paper.pdf', 'file_path': './Rag_data\\\\cnn_paper.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:22:47+00:00', 'trapped': '', 'modDate': \"D:20250522022247+00'00'\", 'creationDate': \"D:20250522022247+00'00'\", 'page': 0}, page_content='Convolutional Neural Networks (CNNs) are specialized neural networks for processing data with\\ngrid-like topology, such as images.\\nCNNs use convolutional layers to scan input features with filters, detecting spatial hierarchies.\\nKey components include convolutional layers, pooling layers, and fully connected layers.\\nActivation functions such as ReLU introduce non-linearity.\\nCNNs reduce the number of parameters via weight sharing, improving efficiency.\\nPopular architectures include LeNet, AlexNet, VGG, GoogLeNet, and ResNet.\\nCNNs are widely used in image recognition, segmentation, and video analysis.\\nConvolutional Neural Networks (CNNs) are specialized neural networks for processing data with\\ngrid-like topology, such as images.\\nCNNs use convolutional layers to scan input features with filters, detecting spatial hierarchies.\\nKey components include convolutional layers, pooling layers, and fully connected layers.\\nActivation functions such as ReLU introduce non-linearity.\\nCNNs reduce the number of parameters via weight sharing, improving efficiency.\\nPopular architectures include LeNet, AlexNet, VGG, GoogLeNet, and ResNet.\\nCNNs are widely used in image recognition, segmentation, and video analysis.\\nConvolutional Neural Networks (CNNs) are specialized neural networks for processing data with\\ngrid-like topology, such as images.\\nCNNs use convolutional layers to scan input features with filters, detecting spatial hierarchies.\\nKey components include convolutional layers, pooling layers, and fully connected layers.\\nActivation functions such as ReLU introduce non-linearity.\\nCNNs reduce the number of parameters via weight sharing, improving efficiency.\\nPopular architectures include LeNet, AlexNet, VGG, GoogLeNet, and ResNet.\\nCNNs are widely used in image recognition, segmentation, and video analysis.\\nConvolutional Neural Networks (CNNs) are specialized neural networks for processing data with\\ngrid-like topology, such as images.\\nCNNs use convolutional layers to scan input features with filters, detecting spatial hierarchies.\\nKey components include convolutional layers, pooling layers, and fully connected layers.\\nActivation functions such as ReLU introduce non-linearity.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:22:47+00:00', 'source': './Rag_data\\\\cnn_paper.pdf', 'file_path': './Rag_data\\\\cnn_paper.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:22:47+00:00', 'trapped': '', 'modDate': \"D:20250522022247+00'00'\", 'creationDate': \"D:20250522022247+00'00'\", 'page': 1}, page_content='CNNs reduce the number of parameters via weight sharing, improving efficiency.\\nPopular architectures include LeNet, AlexNet, VGG, GoogLeNet, and ResNet.\\nCNNs are widely used in image recognition, segmentation, and video analysis.\\nConvolutional Neural Networks (CNNs) are specialized neural networks for processing data with\\ngrid-like topology, such as images.\\nCNNs use convolutional layers to scan input features with filters, detecting spatial hierarchies.\\nKey components include convolutional layers, pooling layers, and fully connected layers.\\nActivation functions such as ReLU introduce non-linearity.\\nCNNs reduce the number of parameters via weight sharing, improving efficiency.\\nPopular architectures include LeNet, AlexNet, VGG, GoogLeNet, and ResNet.\\nCNNs are widely used in image recognition, segmentation, and video analysis.\\nConvolutional Neural Networks (CNNs) are specialized neural networks for processing data with\\ngrid-like topology, such as images.\\nCNNs use convolutional layers to scan input features with filters, detecting spatial hierarchies.\\nKey components include convolutional layers, pooling layers, and fully connected layers.\\nActivation functions such as ReLU introduce non-linearity.\\nCNNs reduce the number of parameters via weight sharing, improving efficiency.\\nPopular architectures include LeNet, AlexNet, VGG, GoogLeNet, and ResNet.\\nCNNs are widely used in image recognition, segmentation, and video analysis.\\nConvolutional Neural Networks (CNNs) are specialized neural networks for processing data with\\ngrid-like topology, such as images.\\nCNNs use convolutional layers to scan input features with filters, detecting spatial hierarchies.\\nKey components include convolutional layers, pooling layers, and fully connected layers.\\nActivation functions such as ReLU introduce non-linearity.\\nCNNs reduce the number of parameters via weight sharing, improving efficiency.\\nPopular architectures include LeNet, AlexNet, VGG, GoogLeNet, and ResNet.\\nCNNs are widely used in image recognition, segmentation, and video analysis.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:22:47+00:00', 'source': './Rag_data\\\\cnn_paper.pdf', 'file_path': './Rag_data\\\\cnn_paper.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:22:47+00:00', 'trapped': '', 'modDate': \"D:20250522022247+00'00'\", 'creationDate': \"D:20250522022247+00'00'\", 'page': 2}, page_content='Convolutional Neural Networks (CNNs) are specialized neural networks for processing data with\\ngrid-like topology, such as images.\\nCNNs use convolutional layers to scan input features with filters, detecting spatial hierarchies.\\nKey components include convolutional layers, pooling layers, and fully connected layers.\\nActivation functions such as ReLU introduce non-linearity.\\nCNNs reduce the number of parameters via weight sharing, improving efficiency.\\nPopular architectures include LeNet, AlexNet, VGG, GoogLeNet, and ResNet.\\nCNNs are widely used in image recognition, segmentation, and video analysis.\\nConvolutional Neural Networks (CNNs) are specialized neural networks for processing data with\\ngrid-like topology, such as images.\\nCNNs use convolutional layers to scan input features with filters, detecting spatial hierarchies.\\nKey components include convolutional layers, pooling layers, and fully connected layers.\\nActivation functions such as ReLU introduce non-linearity.\\nCNNs reduce the number of parameters via weight sharing, improving efficiency.\\nPopular architectures include LeNet, AlexNet, VGG, GoogLeNet, and ResNet.\\nCNNs are widely used in image recognition, segmentation, and video analysis.\\nConvolutional Neural Networks (CNNs) are specialized neural networks for processing data with\\ngrid-like topology, such as images.\\nCNNs use convolutional layers to scan input features with filters, detecting spatial hierarchies.\\nKey components include convolutional layers, pooling layers, and fully connected layers.\\nActivation functions such as ReLU introduce non-linearity.\\nCNNs reduce the number of parameters via weight sharing, improving efficiency.\\nPopular architectures include LeNet, AlexNet, VGG, GoogLeNet, and ResNet.\\nCNNs are widely used in image recognition, segmentation, and video analysis.\\nConvolutional Neural Networks (CNNs) are specialized neural networks for processing data with\\ngrid-like topology, such as images.\\nCNNs use convolutional layers to scan input features with filters, detecting spatial hierarchies.\\nKey components include convolutional layers, pooling layers, and fully connected layers.\\nActivation functions such as ReLU introduce non-linearity.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:22:47+00:00', 'source': './Rag_data\\\\cnn_paper.pdf', 'file_path': './Rag_data\\\\cnn_paper.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:22:47+00:00', 'trapped': '', 'modDate': \"D:20250522022247+00'00'\", 'creationDate': \"D:20250522022247+00'00'\", 'page': 3}, page_content='CNNs reduce the number of parameters via weight sharing, improving efficiency.\\nPopular architectures include LeNet, AlexNet, VGG, GoogLeNet, and ResNet.\\nCNNs are widely used in image recognition, segmentation, and video analysis.\\nConvolutional Neural Networks (CNNs) are specialized neural networks for processing data with\\ngrid-like topology, such as images.\\nCNNs use convolutional layers to scan input features with filters, detecting spatial hierarchies.\\nKey components include convolutional layers, pooling layers, and fully connected layers.\\nActivation functions such as ReLU introduce non-linearity.\\nCNNs reduce the number of parameters via weight sharing, improving efficiency.\\nPopular architectures include LeNet, AlexNet, VGG, GoogLeNet, and ResNet.\\nCNNs are widely used in image recognition, segmentation, and video analysis.\\nConvolutional Neural Networks (CNNs) are specialized neural networks for processing data with\\ngrid-like topology, such as images.\\nCNNs use convolutional layers to scan input features with filters, detecting spatial hierarchies.\\nKey components include convolutional layers, pooling layers, and fully connected layers.\\nActivation functions such as ReLU introduce non-linearity.\\nCNNs reduce the number of parameters via weight sharing, improving efficiency.\\nPopular architectures include LeNet, AlexNet, VGG, GoogLeNet, and ResNet.\\nCNNs are widely used in image recognition, segmentation, and video analysis.\\nConvolutional Neural Networks (CNNs) are specialized neural networks for processing data with\\ngrid-like topology, such as images.\\nCNNs use convolutional layers to scan input features with filters, detecting spatial hierarchies.\\nKey components include convolutional layers, pooling layers, and fully connected layers.\\nActivation functions such as ReLU introduce non-linearity.\\nCNNs reduce the number of parameters via weight sharing, improving efficiency.\\nPopular architectures include LeNet, AlexNet, VGG, GoogLeNet, and ResNet.\\nCNNs are widely used in image recognition, segmentation, and video analysis.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:22:47+00:00', 'source': './Rag_data\\\\cnn_paper.pdf', 'file_path': './Rag_data\\\\cnn_paper.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:22:47+00:00', 'trapped': '', 'modDate': \"D:20250522022247+00'00'\", 'creationDate': \"D:20250522022247+00'00'\", 'page': 4}, page_content='Convolutional Neural Networks (CNNs) are specialized neural networks for processing data with\\ngrid-like topology, such as images.\\nCNNs use convolutional layers to scan input features with filters, detecting spatial hierarchies.\\nKey components include convolutional layers, pooling layers, and fully connected layers.\\nActivation functions such as ReLU introduce non-linearity.\\nCNNs reduce the number of parameters via weight sharing, improving efficiency.\\nPopular architectures include LeNet, AlexNet, VGG, GoogLeNet, and ResNet.\\nCNNs are widely used in image recognition, segmentation, and video analysis.\\nConvolutional Neural Networks (CNNs) are specialized neural networks for processing data with\\ngrid-like topology, such as images.\\nCNNs use convolutional layers to scan input features with filters, detecting spatial hierarchies.\\nKey components include convolutional layers, pooling layers, and fully connected layers.\\nActivation functions such as ReLU introduce non-linearity.\\nCNNs reduce the number of parameters via weight sharing, improving efficiency.\\nPopular architectures include LeNet, AlexNet, VGG, GoogLeNet, and ResNet.\\nCNNs are widely used in image recognition, segmentation, and video analysis.\\nConvolutional Neural Networks (CNNs) are specialized neural networks for processing data with\\ngrid-like topology, such as images.\\nCNNs use convolutional layers to scan input features with filters, detecting spatial hierarchies.\\nKey components include convolutional layers, pooling layers, and fully connected layers.\\nActivation functions such as ReLU introduce non-linearity.\\nCNNs reduce the number of parameters via weight sharing, improving efficiency.\\nPopular architectures include LeNet, AlexNet, VGG, GoogLeNet, and ResNet.\\nCNNs are widely used in image recognition, segmentation, and video analysis.\\nConvolutional Neural Networks (CNNs) are specialized neural networks for processing data with\\ngrid-like topology, such as images.\\nCNNs use convolutional layers to scan input features with filters, detecting spatial hierarchies.\\nKey components include convolutional layers, pooling layers, and fully connected layers.\\nActivation functions such as ReLU introduce non-linearity.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:22:47+00:00', 'source': './Rag_data\\\\cnn_paper.pdf', 'file_path': './Rag_data\\\\cnn_paper.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:22:47+00:00', 'trapped': '', 'modDate': \"D:20250522022247+00'00'\", 'creationDate': \"D:20250522022247+00'00'\", 'page': 5}, page_content='CNNs reduce the number of parameters via weight sharing, improving efficiency.\\nPopular architectures include LeNet, AlexNet, VGG, GoogLeNet, and ResNet.\\nCNNs are widely used in image recognition, segmentation, and video analysis.\\nConvolutional Neural Networks (CNNs) are specialized neural networks for processing data with\\ngrid-like topology, such as images.\\nCNNs use convolutional layers to scan input features with filters, detecting spatial hierarchies.\\nKey components include convolutional layers, pooling layers, and fully connected layers.\\nActivation functions such as ReLU introduce non-linearity.\\nCNNs reduce the number of parameters via weight sharing, improving efficiency.\\nPopular architectures include LeNet, AlexNet, VGG, GoogLeNet, and ResNet.\\nCNNs are widely used in image recognition, segmentation, and video analysis.\\nConvolutional Neural Networks (CNNs) are specialized neural networks for processing data with\\ngrid-like topology, such as images.\\nCNNs use convolutional layers to scan input features with filters, detecting spatial hierarchies.\\nKey components include convolutional layers, pooling layers, and fully connected layers.\\nActivation functions such as ReLU introduce non-linearity.\\nCNNs reduce the number of parameters via weight sharing, improving efficiency.\\nPopular architectures include LeNet, AlexNet, VGG, GoogLeNet, and ResNet.\\nCNNs are widely used in image recognition, segmentation, and video analysis.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:21:33+00:00', 'source': './Rag_data\\\\resnet_paper_structured.pdf', 'file_path': './Rag_data\\\\resnet_paper_structured.pdf', 'total_pages': 12, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:21:33+00:00', 'trapped': '', 'modDate': \"D:20250522022133+00'00'\", 'creationDate': \"D:20250522022133+00'00'\", 'page': 0}, page_content='ResNet (Residual Network) was introduced by Kaiming He et al.\\nin 2015 to address the vanishing gradient problem in deep neural networks.\\nThe core idea behind ResNet is the use of skip connections, also called residual connections, that allow\\ngradients to flow directly through the network by skipping one or more layers.\\nThis architecture enabled the training of networks with more than 100 layers without performance\\ndegradation.\\nResNet architectures are typically structured using residual blocks, which consist of convolutional\\nlayers followed by batch normalization and ReLU activation.\\nEach block includes a shortcut connection that adds the input directly to the output of the convolutional\\nlayers.\\nThis simple yet powerful innovation became the backbone for many computer vision models, including\\nthose used in image classification, object detection, and segmentation.\\nVariants like ResNet-50, ResNet-101, and ResNet-152 offer different depths.\\nThe ResNet-50 model, for example, uses 50 layers and has become a benchmark for deep learning\\nresearch.\\nOne of the major strengths of ResNet is its ability to generalize well with fewer training epochs due to\\nimproved gradient flow.\\nIts success on the ImageNet dataset demonstrated its robustness, and it won the 2015 ImageNet Large\\nScale Visual Recognition Challenge (ILSVRC).\\nResNet (Residual Network) was introduced by Kaiming He et al.\\nin 2015 to address the vanishing gradient problem in deep neural networks.\\nThe core idea behind ResNet is the use of skip connections, also called residual connections, that allow\\ngradients to flow directly through the network by skipping one or more layers.\\nThis architecture enabled the training of networks with more than 100 layers without performance\\ndegradation.\\nResNet architectures are typically structured using residual blocks, which consist of convolutional\\nlayers followed by batch normalization and ReLU activation.\\nEach block includes a shortcut connection that adds the input directly to the output of the convolutional\\nlayers.\\nThis simple yet powerful innovation became the backbone for many computer vision models, including\\nthose used in image classification, object detection, and segmentation.\\nVariants like ResNet-50, ResNet-101, and ResNet-152 offer different depths.\\nThe ResNet-50 model, for example, uses 50 layers and has become a benchmark for deep learning\\nresearch.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:21:33+00:00', 'source': './Rag_data\\\\resnet_paper_structured.pdf', 'file_path': './Rag_data\\\\resnet_paper_structured.pdf', 'total_pages': 12, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:21:33+00:00', 'trapped': '', 'modDate': \"D:20250522022133+00'00'\", 'creationDate': \"D:20250522022133+00'00'\", 'page': 1}, page_content='One of the major strengths of ResNet is its ability to generalize well with fewer training epochs due to\\nimproved gradient flow.\\nIts success on the ImageNet dataset demonstrated its robustness, and it won the 2015 ImageNet Large\\nScale Visual Recognition Challenge (ILSVRC).\\nResNet (Residual Network) was introduced by Kaiming He et al.\\nin 2015 to address the vanishing gradient problem in deep neural networks.\\nThe core idea behind ResNet is the use of skip connections, also called residual connections, that allow\\ngradients to flow directly through the network by skipping one or more layers.\\nThis architecture enabled the training of networks with more than 100 layers without performance\\ndegradation.\\nResNet architectures are typically structured using residual blocks, which consist of convolutional\\nlayers followed by batch normalization and ReLU activation.\\nEach block includes a shortcut connection that adds the input directly to the output of the convolutional\\nlayers.\\nThis simple yet powerful innovation became the backbone for many computer vision models, including\\nthose used in image classification, object detection, and segmentation.\\nVariants like ResNet-50, ResNet-101, and ResNet-152 offer different depths.\\nThe ResNet-50 model, for example, uses 50 layers and has become a benchmark for deep learning\\nresearch.\\nOne of the major strengths of ResNet is its ability to generalize well with fewer training epochs due to\\nimproved gradient flow.\\nIts success on the ImageNet dataset demonstrated its robustness, and it won the 2015 ImageNet Large\\nScale Visual Recognition Challenge (ILSVRC).\\nResNet (Residual Network) was introduced by Kaiming He et al.\\nin 2015 to address the vanishing gradient problem in deep neural networks.\\nThe core idea behind ResNet is the use of skip connections, also called residual connections, that allow\\ngradients to flow directly through the network by skipping one or more layers.\\nThis architecture enabled the training of networks with more than 100 layers without performance\\ndegradation.\\nResNet architectures are typically structured using residual blocks, which consist of convolutional\\nlayers followed by batch normalization and ReLU activation.\\nEach block includes a shortcut connection that adds the input directly to the output of the convolutional\\nlayers.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:21:33+00:00', 'source': './Rag_data\\\\resnet_paper_structured.pdf', 'file_path': './Rag_data\\\\resnet_paper_structured.pdf', 'total_pages': 12, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:21:33+00:00', 'trapped': '', 'modDate': \"D:20250522022133+00'00'\", 'creationDate': \"D:20250522022133+00'00'\", 'page': 2}, page_content='This simple yet powerful innovation became the backbone for many computer vision models, including\\nthose used in image classification, object detection, and segmentation.\\nVariants like ResNet-50, ResNet-101, and ResNet-152 offer different depths.\\nThe ResNet-50 model, for example, uses 50 layers and has become a benchmark for deep learning\\nresearch.\\nOne of the major strengths of ResNet is its ability to generalize well with fewer training epochs due to\\nimproved gradient flow.\\nIts success on the ImageNet dataset demonstrated its robustness, and it won the 2015 ImageNet Large\\nScale Visual Recognition Challenge (ILSVRC).\\nResNet (Residual Network) was introduced by Kaiming He et al.\\nin 2015 to address the vanishing gradient problem in deep neural networks.\\nThe core idea behind ResNet is the use of skip connections, also called residual connections, that allow\\ngradients to flow directly through the network by skipping one or more layers.\\nThis architecture enabled the training of networks with more than 100 layers without performance\\ndegradation.\\nResNet architectures are typically structured using residual blocks, which consist of convolutional\\nlayers followed by batch normalization and ReLU activation.\\nEach block includes a shortcut connection that adds the input directly to the output of the convolutional\\nlayers.\\nThis simple yet powerful innovation became the backbone for many computer vision models, including\\nthose used in image classification, object detection, and segmentation.\\nVariants like ResNet-50, ResNet-101, and ResNet-152 offer different depths.\\nThe ResNet-50 model, for example, uses 50 layers and has become a benchmark for deep learning\\nresearch.\\nOne of the major strengths of ResNet is its ability to generalize well with fewer training epochs due to\\nimproved gradient flow.\\nIts success on the ImageNet dataset demonstrated its robustness, and it won the 2015 ImageNet Large\\nScale Visual Recognition Challenge (ILSVRC).\\nResNet (Residual Network) was introduced by Kaiming He et al.\\nin 2015 to address the vanishing gradient problem in deep neural networks.\\nThe core idea behind ResNet is the use of skip connections, also called residual connections, that allow\\ngradients to flow directly through the network by skipping one or more layers.\\nThis architecture enabled the training of networks with more than 100 layers without performance\\ndegradation.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:21:33+00:00', 'source': './Rag_data\\\\resnet_paper_structured.pdf', 'file_path': './Rag_data\\\\resnet_paper_structured.pdf', 'total_pages': 12, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:21:33+00:00', 'trapped': '', 'modDate': \"D:20250522022133+00'00'\", 'creationDate': \"D:20250522022133+00'00'\", 'page': 3}, page_content='ResNet architectures are typically structured using residual blocks, which consist of convolutional\\nlayers followed by batch normalization and ReLU activation.\\nEach block includes a shortcut connection that adds the input directly to the output of the convolutional\\nlayers.\\nThis simple yet powerful innovation became the backbone for many computer vision models, including\\nthose used in image classification, object detection, and segmentation.\\nVariants like ResNet-50, ResNet-101, and ResNet-152 offer different depths.\\nThe ResNet-50 model, for example, uses 50 layers and has become a benchmark for deep learning\\nresearch.\\nOne of the major strengths of ResNet is its ability to generalize well with fewer training epochs due to\\nimproved gradient flow.\\nIts success on the ImageNet dataset demonstrated its robustness, and it won the 2015 ImageNet Large\\nScale Visual Recognition Challenge (ILSVRC).\\nResNet (Residual Network) was introduced by Kaiming He et al.\\nin 2015 to address the vanishing gradient problem in deep neural networks.\\nThe core idea behind ResNet is the use of skip connections, also called residual connections, that allow\\ngradients to flow directly through the network by skipping one or more layers.\\nThis architecture enabled the training of networks with more than 100 layers without performance\\ndegradation.\\nResNet architectures are typically structured using residual blocks, which consist of convolutional\\nlayers followed by batch normalization and ReLU activation.\\nEach block includes a shortcut connection that adds the input directly to the output of the convolutional\\nlayers.\\nThis simple yet powerful innovation became the backbone for many computer vision models, including\\nthose used in image classification, object detection, and segmentation.\\nVariants like ResNet-50, ResNet-101, and ResNet-152 offer different depths.\\nThe ResNet-50 model, for example, uses 50 layers and has become a benchmark for deep learning\\nresearch.\\nOne of the major strengths of ResNet is its ability to generalize well with fewer training epochs due to\\nimproved gradient flow.\\nIts success on the ImageNet dataset demonstrated its robustness, and it won the 2015 ImageNet Large\\nScale Visual Recognition Challenge (ILSVRC).\\nResNet (Residual Network) was introduced by Kaiming He et al.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:21:33+00:00', 'source': './Rag_data\\\\resnet_paper_structured.pdf', 'file_path': './Rag_data\\\\resnet_paper_structured.pdf', 'total_pages': 12, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:21:33+00:00', 'trapped': '', 'modDate': \"D:20250522022133+00'00'\", 'creationDate': \"D:20250522022133+00'00'\", 'page': 4}, page_content='in 2015 to address the vanishing gradient problem in deep neural networks.\\nThe core idea behind ResNet is the use of skip connections, also called residual connections, that allow\\ngradients to flow directly through the network by skipping one or more layers.\\nThis architecture enabled the training of networks with more than 100 layers without performance\\ndegradation.\\nResNet architectures are typically structured using residual blocks, which consist of convolutional\\nlayers followed by batch normalization and ReLU activation.\\nEach block includes a shortcut connection that adds the input directly to the output of the convolutional\\nlayers.\\nThis simple yet powerful innovation became the backbone for many computer vision models, including\\nthose used in image classification, object detection, and segmentation.\\nVariants like ResNet-50, ResNet-101, and ResNet-152 offer different depths.\\nThe ResNet-50 model, for example, uses 50 layers and has become a benchmark for deep learning\\nresearch.\\nOne of the major strengths of ResNet is its ability to generalize well with fewer training epochs due to\\nimproved gradient flow.\\nIts success on the ImageNet dataset demonstrated its robustness, and it won the 2015 ImageNet Large\\nScale Visual Recognition Challenge (ILSVRC).\\nResNet (Residual Network) was introduced by Kaiming He et al.\\nin 2015 to address the vanishing gradient problem in deep neural networks.\\nThe core idea behind ResNet is the use of skip connections, also called residual connections, that allow\\ngradients to flow directly through the network by skipping one or more layers.\\nThis architecture enabled the training of networks with more than 100 layers without performance\\ndegradation.\\nResNet architectures are typically structured using residual blocks, which consist of convolutional\\nlayers followed by batch normalization and ReLU activation.\\nEach block includes a shortcut connection that adds the input directly to the output of the convolutional\\nlayers.\\nThis simple yet powerful innovation became the backbone for many computer vision models, including\\nthose used in image classification, object detection, and segmentation.\\nVariants like ResNet-50, ResNet-101, and ResNet-152 offer different depths.\\nThe ResNet-50 model, for example, uses 50 layers and has become a benchmark for deep learning\\nresearch.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:21:33+00:00', 'source': './Rag_data\\\\resnet_paper_structured.pdf', 'file_path': './Rag_data\\\\resnet_paper_structured.pdf', 'total_pages': 12, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:21:33+00:00', 'trapped': '', 'modDate': \"D:20250522022133+00'00'\", 'creationDate': \"D:20250522022133+00'00'\", 'page': 5}, page_content='One of the major strengths of ResNet is its ability to generalize well with fewer training epochs due to\\nimproved gradient flow.\\nIts success on the ImageNet dataset demonstrated its robustness, and it won the 2015 ImageNet Large\\nScale Visual Recognition Challenge (ILSVRC).\\nResNet (Residual Network) was introduced by Kaiming He et al.\\nin 2015 to address the vanishing gradient problem in deep neural networks.\\nThe core idea behind ResNet is the use of skip connections, also called residual connections, that allow\\ngradients to flow directly through the network by skipping one or more layers.\\nThis architecture enabled the training of networks with more than 100 layers without performance\\ndegradation.\\nResNet architectures are typically structured using residual blocks, which consist of convolutional\\nlayers followed by batch normalization and ReLU activation.\\nEach block includes a shortcut connection that adds the input directly to the output of the convolutional\\nlayers.\\nThis simple yet powerful innovation became the backbone for many computer vision models, including\\nthose used in image classification, object detection, and segmentation.\\nVariants like ResNet-50, ResNet-101, and ResNet-152 offer different depths.\\nThe ResNet-50 model, for example, uses 50 layers and has become a benchmark for deep learning\\nresearch.\\nOne of the major strengths of ResNet is its ability to generalize well with fewer training epochs due to\\nimproved gradient flow.\\nIts success on the ImageNet dataset demonstrated its robustness, and it won the 2015 ImageNet Large\\nScale Visual Recognition Challenge (ILSVRC).\\nResNet (Residual Network) was introduced by Kaiming He et al.\\nin 2015 to address the vanishing gradient problem in deep neural networks.\\nThe core idea behind ResNet is the use of skip connections, also called residual connections, that allow\\ngradients to flow directly through the network by skipping one or more layers.\\nThis architecture enabled the training of networks with more than 100 layers without performance\\ndegradation.\\nResNet architectures are typically structured using residual blocks, which consist of convolutional\\nlayers followed by batch normalization and ReLU activation.\\nEach block includes a shortcut connection that adds the input directly to the output of the convolutional\\nlayers.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:21:33+00:00', 'source': './Rag_data\\\\resnet_paper_structured.pdf', 'file_path': './Rag_data\\\\resnet_paper_structured.pdf', 'total_pages': 12, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:21:33+00:00', 'trapped': '', 'modDate': \"D:20250522022133+00'00'\", 'creationDate': \"D:20250522022133+00'00'\", 'page': 6}, page_content='This simple yet powerful innovation became the backbone for many computer vision models, including\\nthose used in image classification, object detection, and segmentation.\\nVariants like ResNet-50, ResNet-101, and ResNet-152 offer different depths.\\nThe ResNet-50 model, for example, uses 50 layers and has become a benchmark for deep learning\\nresearch.\\nOne of the major strengths of ResNet is its ability to generalize well with fewer training epochs due to\\nimproved gradient flow.\\nIts success on the ImageNet dataset demonstrated its robustness, and it won the 2015 ImageNet Large\\nScale Visual Recognition Challenge (ILSVRC).\\nResNet (Residual Network) was introduced by Kaiming He et al.\\nin 2015 to address the vanishing gradient problem in deep neural networks.\\nThe core idea behind ResNet is the use of skip connections, also called residual connections, that allow\\ngradients to flow directly through the network by skipping one or more layers.\\nThis architecture enabled the training of networks with more than 100 layers without performance\\ndegradation.\\nResNet architectures are typically structured using residual blocks, which consist of convolutional\\nlayers followed by batch normalization and ReLU activation.\\nEach block includes a shortcut connection that adds the input directly to the output of the convolutional\\nlayers.\\nThis simple yet powerful innovation became the backbone for many computer vision models, including\\nthose used in image classification, object detection, and segmentation.\\nVariants like ResNet-50, ResNet-101, and ResNet-152 offer different depths.\\nThe ResNet-50 model, for example, uses 50 layers and has become a benchmark for deep learning\\nresearch.\\nOne of the major strengths of ResNet is its ability to generalize well with fewer training epochs due to\\nimproved gradient flow.\\nIts success on the ImageNet dataset demonstrated its robustness, and it won the 2015 ImageNet Large\\nScale Visual Recognition Challenge (ILSVRC).\\nResNet (Residual Network) was introduced by Kaiming He et al.\\nin 2015 to address the vanishing gradient problem in deep neural networks.\\nThe core idea behind ResNet is the use of skip connections, also called residual connections, that allow\\ngradients to flow directly through the network by skipping one or more layers.\\nThis architecture enabled the training of networks with more than 100 layers without performance\\ndegradation.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:21:33+00:00', 'source': './Rag_data\\\\resnet_paper_structured.pdf', 'file_path': './Rag_data\\\\resnet_paper_structured.pdf', 'total_pages': 12, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:21:33+00:00', 'trapped': '', 'modDate': \"D:20250522022133+00'00'\", 'creationDate': \"D:20250522022133+00'00'\", 'page': 7}, page_content='ResNet architectures are typically structured using residual blocks, which consist of convolutional\\nlayers followed by batch normalization and ReLU activation.\\nEach block includes a shortcut connection that adds the input directly to the output of the convolutional\\nlayers.\\nThis simple yet powerful innovation became the backbone for many computer vision models, including\\nthose used in image classification, object detection, and segmentation.\\nVariants like ResNet-50, ResNet-101, and ResNet-152 offer different depths.\\nThe ResNet-50 model, for example, uses 50 layers and has become a benchmark for deep learning\\nresearch.\\nOne of the major strengths of ResNet is its ability to generalize well with fewer training epochs due to\\nimproved gradient flow.\\nIts success on the ImageNet dataset demonstrated its robustness, and it won the 2015 ImageNet Large\\nScale Visual Recognition Challenge (ILSVRC).\\nResNet (Residual Network) was introduced by Kaiming He et al.\\nin 2015 to address the vanishing gradient problem in deep neural networks.\\nThe core idea behind ResNet is the use of skip connections, also called residual connections, that allow\\ngradients to flow directly through the network by skipping one or more layers.\\nThis architecture enabled the training of networks with more than 100 layers without performance\\ndegradation.\\nResNet architectures are typically structured using residual blocks, which consist of convolutional\\nlayers followed by batch normalization and ReLU activation.\\nEach block includes a shortcut connection that adds the input directly to the output of the convolutional\\nlayers.\\nThis simple yet powerful innovation became the backbone for many computer vision models, including\\nthose used in image classification, object detection, and segmentation.\\nVariants like ResNet-50, ResNet-101, and ResNet-152 offer different depths.\\nThe ResNet-50 model, for example, uses 50 layers and has become a benchmark for deep learning\\nresearch.\\nOne of the major strengths of ResNet is its ability to generalize well with fewer training epochs due to\\nimproved gradient flow.\\nIts success on the ImageNet dataset demonstrated its robustness, and it won the 2015 ImageNet Large\\nScale Visual Recognition Challenge (ILSVRC).\\nResNet (Residual Network) was introduced by Kaiming He et al.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:21:33+00:00', 'source': './Rag_data\\\\resnet_paper_structured.pdf', 'file_path': './Rag_data\\\\resnet_paper_structured.pdf', 'total_pages': 12, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:21:33+00:00', 'trapped': '', 'modDate': \"D:20250522022133+00'00'\", 'creationDate': \"D:20250522022133+00'00'\", 'page': 8}, page_content='in 2015 to address the vanishing gradient problem in deep neural networks.\\nThe core idea behind ResNet is the use of skip connections, also called residual connections, that allow\\ngradients to flow directly through the network by skipping one or more layers.\\nThis architecture enabled the training of networks with more than 100 layers without performance\\ndegradation.\\nResNet architectures are typically structured using residual blocks, which consist of convolutional\\nlayers followed by batch normalization and ReLU activation.\\nEach block includes a shortcut connection that adds the input directly to the output of the convolutional\\nlayers.\\nThis simple yet powerful innovation became the backbone for many computer vision models, including\\nthose used in image classification, object detection, and segmentation.\\nVariants like ResNet-50, ResNet-101, and ResNet-152 offer different depths.\\nThe ResNet-50 model, for example, uses 50 layers and has become a benchmark for deep learning\\nresearch.\\nOne of the major strengths of ResNet is its ability to generalize well with fewer training epochs due to\\nimproved gradient flow.\\nIts success on the ImageNet dataset demonstrated its robustness, and it won the 2015 ImageNet Large\\nScale Visual Recognition Challenge (ILSVRC).\\nResNet (Residual Network) was introduced by Kaiming He et al.\\nin 2015 to address the vanishing gradient problem in deep neural networks.\\nThe core idea behind ResNet is the use of skip connections, also called residual connections, that allow\\ngradients to flow directly through the network by skipping one or more layers.\\nThis architecture enabled the training of networks with more than 100 layers without performance\\ndegradation.\\nResNet architectures are typically structured using residual blocks, which consist of convolutional\\nlayers followed by batch normalization and ReLU activation.\\nEach block includes a shortcut connection that adds the input directly to the output of the convolutional\\nlayers.\\nThis simple yet powerful innovation became the backbone for many computer vision models, including\\nthose used in image classification, object detection, and segmentation.\\nVariants like ResNet-50, ResNet-101, and ResNet-152 offer different depths.\\nThe ResNet-50 model, for example, uses 50 layers and has become a benchmark for deep learning\\nresearch.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:21:33+00:00', 'source': './Rag_data\\\\resnet_paper_structured.pdf', 'file_path': './Rag_data\\\\resnet_paper_structured.pdf', 'total_pages': 12, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:21:33+00:00', 'trapped': '', 'modDate': \"D:20250522022133+00'00'\", 'creationDate': \"D:20250522022133+00'00'\", 'page': 9}, page_content='One of the major strengths of ResNet is its ability to generalize well with fewer training epochs due to\\nimproved gradient flow.\\nIts success on the ImageNet dataset demonstrated its robustness, and it won the 2015 ImageNet Large\\nScale Visual Recognition Challenge (ILSVRC).\\nResNet (Residual Network) was introduced by Kaiming He et al.\\nin 2015 to address the vanishing gradient problem in deep neural networks.\\nThe core idea behind ResNet is the use of skip connections, also called residual connections, that allow\\ngradients to flow directly through the network by skipping one or more layers.\\nThis architecture enabled the training of networks with more than 100 layers without performance\\ndegradation.\\nResNet architectures are typically structured using residual blocks, which consist of convolutional\\nlayers followed by batch normalization and ReLU activation.\\nEach block includes a shortcut connection that adds the input directly to the output of the convolutional\\nlayers.\\nThis simple yet powerful innovation became the backbone for many computer vision models, including\\nthose used in image classification, object detection, and segmentation.\\nVariants like ResNet-50, ResNet-101, and ResNet-152 offer different depths.\\nThe ResNet-50 model, for example, uses 50 layers and has become a benchmark for deep learning\\nresearch.\\nOne of the major strengths of ResNet is its ability to generalize well with fewer training epochs due to\\nimproved gradient flow.\\nIts success on the ImageNet dataset demonstrated its robustness, and it won the 2015 ImageNet Large\\nScale Visual Recognition Challenge (ILSVRC).\\nResNet (Residual Network) was introduced by Kaiming He et al.\\nin 2015 to address the vanishing gradient problem in deep neural networks.\\nThe core idea behind ResNet is the use of skip connections, also called residual connections, that allow\\ngradients to flow directly through the network by skipping one or more layers.\\nThis architecture enabled the training of networks with more than 100 layers without performance\\ndegradation.\\nResNet architectures are typically structured using residual blocks, which consist of convolutional\\nlayers followed by batch normalization and ReLU activation.\\nEach block includes a shortcut connection that adds the input directly to the output of the convolutional\\nlayers.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:21:33+00:00', 'source': './Rag_data\\\\resnet_paper_structured.pdf', 'file_path': './Rag_data\\\\resnet_paper_structured.pdf', 'total_pages': 12, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:21:33+00:00', 'trapped': '', 'modDate': \"D:20250522022133+00'00'\", 'creationDate': \"D:20250522022133+00'00'\", 'page': 10}, page_content='This simple yet powerful innovation became the backbone for many computer vision models, including\\nthose used in image classification, object detection, and segmentation.\\nVariants like ResNet-50, ResNet-101, and ResNet-152 offer different depths.\\nThe ResNet-50 model, for example, uses 50 layers and has become a benchmark for deep learning\\nresearch.\\nOne of the major strengths of ResNet is its ability to generalize well with fewer training epochs due to\\nimproved gradient flow.\\nIts success on the ImageNet dataset demonstrated its robustness, and it won the 2015 ImageNet Large\\nScale Visual Recognition Challenge (ILSVRC).\\nResNet (Residual Network) was introduced by Kaiming He et al.\\nin 2015 to address the vanishing gradient problem in deep neural networks.\\nThe core idea behind ResNet is the use of skip connections, also called residual connections, that allow\\ngradients to flow directly through the network by skipping one or more layers.\\nThis architecture enabled the training of networks with more than 100 layers without performance\\ndegradation.\\nResNet architectures are typically structured using residual blocks, which consist of convolutional\\nlayers followed by batch normalization and ReLU activation.\\nEach block includes a shortcut connection that adds the input directly to the output of the convolutional\\nlayers.\\nThis simple yet powerful innovation became the backbone for many computer vision models, including\\nthose used in image classification, object detection, and segmentation.\\nVariants like ResNet-50, ResNet-101, and ResNet-152 offer different depths.\\nThe ResNet-50 model, for example, uses 50 layers and has become a benchmark for deep learning\\nresearch.\\nOne of the major strengths of ResNet is its ability to generalize well with fewer training epochs due to\\nimproved gradient flow.\\nIts success on the ImageNet dataset demonstrated its robustness, and it won the 2015 ImageNet Large\\nScale Visual Recognition Challenge (ILSVRC).\\nResNet (Residual Network) was introduced by Kaiming He et al.\\nin 2015 to address the vanishing gradient problem in deep neural networks.\\nThe core idea behind ResNet is the use of skip connections, also called residual connections, that allow\\ngradients to flow directly through the network by skipping one or more layers.\\nThis architecture enabled the training of networks with more than 100 layers without performance\\ndegradation.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:21:33+00:00', 'source': './Rag_data\\\\resnet_paper_structured.pdf', 'file_path': './Rag_data\\\\resnet_paper_structured.pdf', 'total_pages': 12, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:21:33+00:00', 'trapped': '', 'modDate': \"D:20250522022133+00'00'\", 'creationDate': \"D:20250522022133+00'00'\", 'page': 11}, page_content='ResNet architectures are typically structured using residual blocks, which consist of convolutional\\nlayers followed by batch normalization and ReLU activation.\\nEach block includes a shortcut connection that adds the input directly to the output of the convolutional\\nlayers.\\nThis simple yet powerful innovation became the backbone for many computer vision models, including\\nthose used in image classification, object detection, and segmentation.\\nVariants like ResNet-50, ResNet-101, and ResNet-152 offer different depths.\\nThe ResNet-50 model, for example, uses 50 layers and has become a benchmark for deep learning\\nresearch.\\nOne of the major strengths of ResNet is its ability to generalize well with fewer training epochs due to\\nimproved gradient flow.\\nIts success on the ImageNet dataset demonstrated its robustness, and it won the 2015 ImageNet Large\\nScale Visual Recognition Challenge (ILSVRC).'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:22:47+00:00', 'source': './Rag_data\\\\transformer_paper.pdf', 'file_path': './Rag_data\\\\transformer_paper.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:22:47+00:00', 'trapped': '', 'modDate': \"D:20250522022247+00'00'\", 'creationDate': \"D:20250522022247+00'00'\", 'page': 0}, page_content='The Transformer architecture, introduced in 2017 by Vaswani et al., is built entirely on attention\\nmechanisms, removing recurrence entirely.\\nIt consists of an encoder-decoder structure where both the encoder and decoder are made up of layers\\ncontaining multi-head self-attention and feed-forward networks.\\nTransformers leverage positional encoding to retain order information in sequences.\\nThe architecture allows for better parallelization and efficiency during training compared to RNNs.\\nTransformers form the backbone of modern NLP models such as BERT, GPT, T5, and more.\\nThey have also been adapted for use in image processing, audio modeling, and multi-modal tasks.\\nThe Transformer architecture, introduced in 2017 by Vaswani et al., is built entirely on attention\\nmechanisms, removing recurrence entirely.\\nIt consists of an encoder-decoder structure where both the encoder and decoder are made up of layers\\ncontaining multi-head self-attention and feed-forward networks.\\nTransformers leverage positional encoding to retain order information in sequences.\\nThe architecture allows for better parallelization and efficiency during training compared to RNNs.\\nTransformers form the backbone of modern NLP models such as BERT, GPT, T5, and more.\\nThey have also been adapted for use in image processing, audio modeling, and multi-modal tasks.\\nThe Transformer architecture, introduced in 2017 by Vaswani et al., is built entirely on attention\\nmechanisms, removing recurrence entirely.\\nIt consists of an encoder-decoder structure where both the encoder and decoder are made up of layers\\ncontaining multi-head self-attention and feed-forward networks.\\nTransformers leverage positional encoding to retain order information in sequences.\\nThe architecture allows for better parallelization and efficiency during training compared to RNNs.\\nTransformers form the backbone of modern NLP models such as BERT, GPT, T5, and more.\\nThey have also been adapted for use in image processing, audio modeling, and multi-modal tasks.\\nThe Transformer architecture, introduced in 2017 by Vaswani et al., is built entirely on attention\\nmechanisms, removing recurrence entirely.\\nIt consists of an encoder-decoder structure where both the encoder and decoder are made up of layers\\ncontaining multi-head self-attention and feed-forward networks.\\nTransformers leverage positional encoding to retain order information in sequences.\\nThe architecture allows for better parallelization and efficiency during training compared to RNNs.\\nTransformers form the backbone of modern NLP models such as BERT, GPT, T5, and more.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:22:47+00:00', 'source': './Rag_data\\\\transformer_paper.pdf', 'file_path': './Rag_data\\\\transformer_paper.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:22:47+00:00', 'trapped': '', 'modDate': \"D:20250522022247+00'00'\", 'creationDate': \"D:20250522022247+00'00'\", 'page': 1}, page_content='They have also been adapted for use in image processing, audio modeling, and multi-modal tasks.\\nThe Transformer architecture, introduced in 2017 by Vaswani et al., is built entirely on attention\\nmechanisms, removing recurrence entirely.\\nIt consists of an encoder-decoder structure where both the encoder and decoder are made up of layers\\ncontaining multi-head self-attention and feed-forward networks.\\nTransformers leverage positional encoding to retain order information in sequences.\\nThe architecture allows for better parallelization and efficiency during training compared to RNNs.\\nTransformers form the backbone of modern NLP models such as BERT, GPT, T5, and more.\\nThey have also been adapted for use in image processing, audio modeling, and multi-modal tasks.\\nThe Transformer architecture, introduced in 2017 by Vaswani et al., is built entirely on attention\\nmechanisms, removing recurrence entirely.\\nIt consists of an encoder-decoder structure where both the encoder and decoder are made up of layers\\ncontaining multi-head self-attention and feed-forward networks.\\nTransformers leverage positional encoding to retain order information in sequences.\\nThe architecture allows for better parallelization and efficiency during training compared to RNNs.\\nTransformers form the backbone of modern NLP models such as BERT, GPT, T5, and more.\\nThey have also been adapted for use in image processing, audio modeling, and multi-modal tasks.\\nThe Transformer architecture, introduced in 2017 by Vaswani et al., is built entirely on attention\\nmechanisms, removing recurrence entirely.\\nIt consists of an encoder-decoder structure where both the encoder and decoder are made up of layers\\ncontaining multi-head self-attention and feed-forward networks.\\nTransformers leverage positional encoding to retain order information in sequences.\\nThe architecture allows for better parallelization and efficiency during training compared to RNNs.\\nTransformers form the backbone of modern NLP models such as BERT, GPT, T5, and more.\\nThey have also been adapted for use in image processing, audio modeling, and multi-modal tasks.\\nThe Transformer architecture, introduced in 2017 by Vaswani et al., is built entirely on attention\\nmechanisms, removing recurrence entirely.\\nIt consists of an encoder-decoder structure where both the encoder and decoder are made up of layers\\ncontaining multi-head self-attention and feed-forward networks.\\nTransformers leverage positional encoding to retain order information in sequences.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:22:47+00:00', 'source': './Rag_data\\\\transformer_paper.pdf', 'file_path': './Rag_data\\\\transformer_paper.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:22:47+00:00', 'trapped': '', 'modDate': \"D:20250522022247+00'00'\", 'creationDate': \"D:20250522022247+00'00'\", 'page': 2}, page_content='The architecture allows for better parallelization and efficiency during training compared to RNNs.\\nTransformers form the backbone of modern NLP models such as BERT, GPT, T5, and more.\\nThey have also been adapted for use in image processing, audio modeling, and multi-modal tasks.\\nThe Transformer architecture, introduced in 2017 by Vaswani et al., is built entirely on attention\\nmechanisms, removing recurrence entirely.\\nIt consists of an encoder-decoder structure where both the encoder and decoder are made up of layers\\ncontaining multi-head self-attention and feed-forward networks.\\nTransformers leverage positional encoding to retain order information in sequences.\\nThe architecture allows for better parallelization and efficiency during training compared to RNNs.\\nTransformers form the backbone of modern NLP models such as BERT, GPT, T5, and more.\\nThey have also been adapted for use in image processing, audio modeling, and multi-modal tasks.\\nThe Transformer architecture, introduced in 2017 by Vaswani et al., is built entirely on attention\\nmechanisms, removing recurrence entirely.\\nIt consists of an encoder-decoder structure where both the encoder and decoder are made up of layers\\ncontaining multi-head self-attention and feed-forward networks.\\nTransformers leverage positional encoding to retain order information in sequences.\\nThe architecture allows for better parallelization and efficiency during training compared to RNNs.\\nTransformers form the backbone of modern NLP models such as BERT, GPT, T5, and more.\\nThey have also been adapted for use in image processing, audio modeling, and multi-modal tasks.\\nThe Transformer architecture, introduced in 2017 by Vaswani et al., is built entirely on attention\\nmechanisms, removing recurrence entirely.\\nIt consists of an encoder-decoder structure where both the encoder and decoder are made up of layers\\ncontaining multi-head self-attention and feed-forward networks.\\nTransformers leverage positional encoding to retain order information in sequences.\\nThe architecture allows for better parallelization and efficiency during training compared to RNNs.\\nTransformers form the backbone of modern NLP models such as BERT, GPT, T5, and more.\\nThey have also been adapted for use in image processing, audio modeling, and multi-modal tasks.\\nThe Transformer architecture, introduced in 2017 by Vaswani et al., is built entirely on attention\\nmechanisms, removing recurrence entirely.\\nIt consists of an encoder-decoder structure where both the encoder and decoder are made up of layers\\ncontaining multi-head self-attention and feed-forward networks.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:22:47+00:00', 'source': './Rag_data\\\\transformer_paper.pdf', 'file_path': './Rag_data\\\\transformer_paper.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:22:47+00:00', 'trapped': '', 'modDate': \"D:20250522022247+00'00'\", 'creationDate': \"D:20250522022247+00'00'\", 'page': 3}, page_content='Transformers leverage positional encoding to retain order information in sequences.\\nThe architecture allows for better parallelization and efficiency during training compared to RNNs.\\nTransformers form the backbone of modern NLP models such as BERT, GPT, T5, and more.\\nThey have also been adapted for use in image processing, audio modeling, and multi-modal tasks.\\nThe Transformer architecture, introduced in 2017 by Vaswani et al., is built entirely on attention\\nmechanisms, removing recurrence entirely.\\nIt consists of an encoder-decoder structure where both the encoder and decoder are made up of layers\\ncontaining multi-head self-attention and feed-forward networks.\\nTransformers leverage positional encoding to retain order information in sequences.\\nThe architecture allows for better parallelization and efficiency during training compared to RNNs.\\nTransformers form the backbone of modern NLP models such as BERT, GPT, T5, and more.\\nThey have also been adapted for use in image processing, audio modeling, and multi-modal tasks.\\nThe Transformer architecture, introduced in 2017 by Vaswani et al., is built entirely on attention\\nmechanisms, removing recurrence entirely.\\nIt consists of an encoder-decoder structure where both the encoder and decoder are made up of layers\\ncontaining multi-head self-attention and feed-forward networks.\\nTransformers leverage positional encoding to retain order information in sequences.\\nThe architecture allows for better parallelization and efficiency during training compared to RNNs.\\nTransformers form the backbone of modern NLP models such as BERT, GPT, T5, and more.\\nThey have also been adapted for use in image processing, audio modeling, and multi-modal tasks.\\nThe Transformer architecture, introduced in 2017 by Vaswani et al., is built entirely on attention\\nmechanisms, removing recurrence entirely.\\nIt consists of an encoder-decoder structure where both the encoder and decoder are made up of layers\\ncontaining multi-head self-attention and feed-forward networks.\\nTransformers leverage positional encoding to retain order information in sequences.\\nThe architecture allows for better parallelization and efficiency during training compared to RNNs.\\nTransformers form the backbone of modern NLP models such as BERT, GPT, T5, and more.\\nThey have also been adapted for use in image processing, audio modeling, and multi-modal tasks.\\nThe Transformer architecture, introduced in 2017 by Vaswani et al., is built entirely on attention\\nmechanisms, removing recurrence entirely.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:22:47+00:00', 'source': './Rag_data\\\\transformer_paper.pdf', 'file_path': './Rag_data\\\\transformer_paper.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:22:47+00:00', 'trapped': '', 'modDate': \"D:20250522022247+00'00'\", 'creationDate': \"D:20250522022247+00'00'\", 'page': 4}, page_content='It consists of an encoder-decoder structure where both the encoder and decoder are made up of layers\\ncontaining multi-head self-attention and feed-forward networks.\\nTransformers leverage positional encoding to retain order information in sequences.\\nThe architecture allows for better parallelization and efficiency during training compared to RNNs.\\nTransformers form the backbone of modern NLP models such as BERT, GPT, T5, and more.\\nThey have also been adapted for use in image processing, audio modeling, and multi-modal tasks.\\nThe Transformer architecture, introduced in 2017 by Vaswani et al., is built entirely on attention\\nmechanisms, removing recurrence entirely.\\nIt consists of an encoder-decoder structure where both the encoder and decoder are made up of layers\\ncontaining multi-head self-attention and feed-forward networks.\\nTransformers leverage positional encoding to retain order information in sequences.\\nThe architecture allows for better parallelization and efficiency during training compared to RNNs.\\nTransformers form the backbone of modern NLP models such as BERT, GPT, T5, and more.\\nThey have also been adapted for use in image processing, audio modeling, and multi-modal tasks.\\nThe Transformer architecture, introduced in 2017 by Vaswani et al., is built entirely on attention\\nmechanisms, removing recurrence entirely.\\nIt consists of an encoder-decoder structure where both the encoder and decoder are made up of layers\\ncontaining multi-head self-attention and feed-forward networks.\\nTransformers leverage positional encoding to retain order information in sequences.\\nThe architecture allows for better parallelization and efficiency during training compared to RNNs.\\nTransformers form the backbone of modern NLP models such as BERT, GPT, T5, and more.\\nThey have also been adapted for use in image processing, audio modeling, and multi-modal tasks.\\nThe Transformer architecture, introduced in 2017 by Vaswani et al., is built entirely on attention\\nmechanisms, removing recurrence entirely.\\nIt consists of an encoder-decoder structure where both the encoder and decoder are made up of layers\\ncontaining multi-head self-attention and feed-forward networks.\\nTransformers leverage positional encoding to retain order information in sequences.\\nThe architecture allows for better parallelization and efficiency during training compared to RNNs.\\nTransformers form the backbone of modern NLP models such as BERT, GPT, T5, and more.\\nThey have also been adapted for use in image processing, audio modeling, and multi-modal tasks.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:22:47+00:00', 'source': './Rag_data\\\\transformer_paper.pdf', 'file_path': './Rag_data\\\\transformer_paper.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:22:47+00:00', 'trapped': '', 'modDate': \"D:20250522022247+00'00'\", 'creationDate': \"D:20250522022247+00'00'\", 'page': 5}, page_content='The Transformer architecture, introduced in 2017 by Vaswani et al., is built entirely on attention\\nmechanisms, removing recurrence entirely.\\nIt consists of an encoder-decoder structure where both the encoder and decoder are made up of layers\\ncontaining multi-head self-attention and feed-forward networks.\\nTransformers leverage positional encoding to retain order information in sequences.\\nThe architecture allows for better parallelization and efficiency during training compared to RNNs.\\nTransformers form the backbone of modern NLP models such as BERT, GPT, T5, and more.\\nThey have also been adapted for use in image processing, audio modeling, and multi-modal tasks.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:22:47+00:00', 'source': './Rag_data\\\\vision_transformer.pdf', 'file_path': './Rag_data\\\\vision_transformer.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:22:47+00:00', 'trapped': '', 'modDate': \"D:20250522022247+00'00'\", 'creationDate': \"D:20250522022247+00'00'\", 'page': 0}, page_content='Vision Transformers (ViT) adapt the Transformer model for computer vision tasks by converting images\\ninto sequences of patches.\\nEach patch is flattened and embedded before being passed to a standard Transformer encoder.\\nThis approach contrasts with CNNs, which process images using localized filters and hierarchical\\nrepresentations.\\nViTs have shown that, given sufficient data and compute, they can outperform traditional CNNs.\\nVariants like DeiT, Swin Transformer, and PiT have improved the efficiency and performance of ViTs.\\nApplications include image classification, object detection, segmentation, and video understanding.\\nVision Transformers (ViT) adapt the Transformer model for computer vision tasks by converting images\\ninto sequences of patches.\\nEach patch is flattened and embedded before being passed to a standard Transformer encoder.\\nThis approach contrasts with CNNs, which process images using localized filters and hierarchical\\nrepresentations.\\nViTs have shown that, given sufficient data and compute, they can outperform traditional CNNs.\\nVariants like DeiT, Swin Transformer, and PiT have improved the efficiency and performance of ViTs.\\nApplications include image classification, object detection, segmentation, and video understanding.\\nVision Transformers (ViT) adapt the Transformer model for computer vision tasks by converting images\\ninto sequences of patches.\\nEach patch is flattened and embedded before being passed to a standard Transformer encoder.\\nThis approach contrasts with CNNs, which process images using localized filters and hierarchical\\nrepresentations.\\nViTs have shown that, given sufficient data and compute, they can outperform traditional CNNs.\\nVariants like DeiT, Swin Transformer, and PiT have improved the efficiency and performance of ViTs.\\nApplications include image classification, object detection, segmentation, and video understanding.\\nVision Transformers (ViT) adapt the Transformer model for computer vision tasks by converting images\\ninto sequences of patches.\\nEach patch is flattened and embedded before being passed to a standard Transformer encoder.\\nThis approach contrasts with CNNs, which process images using localized filters and hierarchical\\nrepresentations.\\nViTs have shown that, given sufficient data and compute, they can outperform traditional CNNs.\\nVariants like DeiT, Swin Transformer, and PiT have improved the efficiency and performance of ViTs.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:22:47+00:00', 'source': './Rag_data\\\\vision_transformer.pdf', 'file_path': './Rag_data\\\\vision_transformer.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:22:47+00:00', 'trapped': '', 'modDate': \"D:20250522022247+00'00'\", 'creationDate': \"D:20250522022247+00'00'\", 'page': 1}, page_content='Applications include image classification, object detection, segmentation, and video understanding.\\nVision Transformers (ViT) adapt the Transformer model for computer vision tasks by converting images\\ninto sequences of patches.\\nEach patch is flattened and embedded before being passed to a standard Transformer encoder.\\nThis approach contrasts with CNNs, which process images using localized filters and hierarchical\\nrepresentations.\\nViTs have shown that, given sufficient data and compute, they can outperform traditional CNNs.\\nVariants like DeiT, Swin Transformer, and PiT have improved the efficiency and performance of ViTs.\\nApplications include image classification, object detection, segmentation, and video understanding.\\nVision Transformers (ViT) adapt the Transformer model for computer vision tasks by converting images\\ninto sequences of patches.\\nEach patch is flattened and embedded before being passed to a standard Transformer encoder.\\nThis approach contrasts with CNNs, which process images using localized filters and hierarchical\\nrepresentations.\\nViTs have shown that, given sufficient data and compute, they can outperform traditional CNNs.\\nVariants like DeiT, Swin Transformer, and PiT have improved the efficiency and performance of ViTs.\\nApplications include image classification, object detection, segmentation, and video understanding.\\nVision Transformers (ViT) adapt the Transformer model for computer vision tasks by converting images\\ninto sequences of patches.\\nEach patch is flattened and embedded before being passed to a standard Transformer encoder.\\nThis approach contrasts with CNNs, which process images using localized filters and hierarchical\\nrepresentations.\\nViTs have shown that, given sufficient data and compute, they can outperform traditional CNNs.\\nVariants like DeiT, Swin Transformer, and PiT have improved the efficiency and performance of ViTs.\\nApplications include image classification, object detection, segmentation, and video understanding.\\nVision Transformers (ViT) adapt the Transformer model for computer vision tasks by converting images\\ninto sequences of patches.\\nEach patch is flattened and embedded before being passed to a standard Transformer encoder.\\nThis approach contrasts with CNNs, which process images using localized filters and hierarchical\\nrepresentations.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:22:47+00:00', 'source': './Rag_data\\\\vision_transformer.pdf', 'file_path': './Rag_data\\\\vision_transformer.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:22:47+00:00', 'trapped': '', 'modDate': \"D:20250522022247+00'00'\", 'creationDate': \"D:20250522022247+00'00'\", 'page': 2}, page_content='ViTs have shown that, given sufficient data and compute, they can outperform traditional CNNs.\\nVariants like DeiT, Swin Transformer, and PiT have improved the efficiency and performance of ViTs.\\nApplications include image classification, object detection, segmentation, and video understanding.\\nVision Transformers (ViT) adapt the Transformer model for computer vision tasks by converting images\\ninto sequences of patches.\\nEach patch is flattened and embedded before being passed to a standard Transformer encoder.\\nThis approach contrasts with CNNs, which process images using localized filters and hierarchical\\nrepresentations.\\nViTs have shown that, given sufficient data and compute, they can outperform traditional CNNs.\\nVariants like DeiT, Swin Transformer, and PiT have improved the efficiency and performance of ViTs.\\nApplications include image classification, object detection, segmentation, and video understanding.\\nVision Transformers (ViT) adapt the Transformer model for computer vision tasks by converting images\\ninto sequences of patches.\\nEach patch is flattened and embedded before being passed to a standard Transformer encoder.\\nThis approach contrasts with CNNs, which process images using localized filters and hierarchical\\nrepresentations.\\nViTs have shown that, given sufficient data and compute, they can outperform traditional CNNs.\\nVariants like DeiT, Swin Transformer, and PiT have improved the efficiency and performance of ViTs.\\nApplications include image classification, object detection, segmentation, and video understanding.\\nVision Transformers (ViT) adapt the Transformer model for computer vision tasks by converting images\\ninto sequences of patches.\\nEach patch is flattened and embedded before being passed to a standard Transformer encoder.\\nThis approach contrasts with CNNs, which process images using localized filters and hierarchical\\nrepresentations.\\nViTs have shown that, given sufficient data and compute, they can outperform traditional CNNs.\\nVariants like DeiT, Swin Transformer, and PiT have improved the efficiency and performance of ViTs.\\nApplications include image classification, object detection, segmentation, and video understanding.\\nVision Transformers (ViT) adapt the Transformer model for computer vision tasks by converting images\\ninto sequences of patches.\\nEach patch is flattened and embedded before being passed to a standard Transformer encoder.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:22:47+00:00', 'source': './Rag_data\\\\vision_transformer.pdf', 'file_path': './Rag_data\\\\vision_transformer.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:22:47+00:00', 'trapped': '', 'modDate': \"D:20250522022247+00'00'\", 'creationDate': \"D:20250522022247+00'00'\", 'page': 3}, page_content='This approach contrasts with CNNs, which process images using localized filters and hierarchical\\nrepresentations.\\nViTs have shown that, given sufficient data and compute, they can outperform traditional CNNs.\\nVariants like DeiT, Swin Transformer, and PiT have improved the efficiency and performance of ViTs.\\nApplications include image classification, object detection, segmentation, and video understanding.\\nVision Transformers (ViT) adapt the Transformer model for computer vision tasks by converting images\\ninto sequences of patches.\\nEach patch is flattened and embedded before being passed to a standard Transformer encoder.\\nThis approach contrasts with CNNs, which process images using localized filters and hierarchical\\nrepresentations.\\nViTs have shown that, given sufficient data and compute, they can outperform traditional CNNs.\\nVariants like DeiT, Swin Transformer, and PiT have improved the efficiency and performance of ViTs.\\nApplications include image classification, object detection, segmentation, and video understanding.\\nVision Transformers (ViT) adapt the Transformer model for computer vision tasks by converting images\\ninto sequences of patches.\\nEach patch is flattened and embedded before being passed to a standard Transformer encoder.\\nThis approach contrasts with CNNs, which process images using localized filters and hierarchical\\nrepresentations.\\nViTs have shown that, given sufficient data and compute, they can outperform traditional CNNs.\\nVariants like DeiT, Swin Transformer, and PiT have improved the efficiency and performance of ViTs.\\nApplications include image classification, object detection, segmentation, and video understanding.\\nVision Transformers (ViT) adapt the Transformer model for computer vision tasks by converting images\\ninto sequences of patches.\\nEach patch is flattened and embedded before being passed to a standard Transformer encoder.\\nThis approach contrasts with CNNs, which process images using localized filters and hierarchical\\nrepresentations.\\nViTs have shown that, given sufficient data and compute, they can outperform traditional CNNs.\\nVariants like DeiT, Swin Transformer, and PiT have improved the efficiency and performance of ViTs.\\nApplications include image classification, object detection, segmentation, and video understanding.\\nVision Transformers (ViT) adapt the Transformer model for computer vision tasks by converting images\\ninto sequences of patches.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:22:47+00:00', 'source': './Rag_data\\\\vision_transformer.pdf', 'file_path': './Rag_data\\\\vision_transformer.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:22:47+00:00', 'trapped': '', 'modDate': \"D:20250522022247+00'00'\", 'creationDate': \"D:20250522022247+00'00'\", 'page': 4}, page_content='Each patch is flattened and embedded before being passed to a standard Transformer encoder.\\nThis approach contrasts with CNNs, which process images using localized filters and hierarchical\\nrepresentations.\\nViTs have shown that, given sufficient data and compute, they can outperform traditional CNNs.\\nVariants like DeiT, Swin Transformer, and PiT have improved the efficiency and performance of ViTs.\\nApplications include image classification, object detection, segmentation, and video understanding.\\nVision Transformers (ViT) adapt the Transformer model for computer vision tasks by converting images\\ninto sequences of patches.\\nEach patch is flattened and embedded before being passed to a standard Transformer encoder.\\nThis approach contrasts with CNNs, which process images using localized filters and hierarchical\\nrepresentations.\\nViTs have shown that, given sufficient data and compute, they can outperform traditional CNNs.\\nVariants like DeiT, Swin Transformer, and PiT have improved the efficiency and performance of ViTs.\\nApplications include image classification, object detection, segmentation, and video understanding.\\nVision Transformers (ViT) adapt the Transformer model for computer vision tasks by converting images\\ninto sequences of patches.\\nEach patch is flattened and embedded before being passed to a standard Transformer encoder.\\nThis approach contrasts with CNNs, which process images using localized filters and hierarchical\\nrepresentations.\\nViTs have shown that, given sufficient data and compute, they can outperform traditional CNNs.\\nVariants like DeiT, Swin Transformer, and PiT have improved the efficiency and performance of ViTs.\\nApplications include image classification, object detection, segmentation, and video understanding.\\nVision Transformers (ViT) adapt the Transformer model for computer vision tasks by converting images\\ninto sequences of patches.\\nEach patch is flattened and embedded before being passed to a standard Transformer encoder.\\nThis approach contrasts with CNNs, which process images using localized filters and hierarchical\\nrepresentations.\\nViTs have shown that, given sufficient data and compute, they can outperform traditional CNNs.\\nVariants like DeiT, Swin Transformer, and PiT have improved the efficiency and performance of ViTs.\\nApplications include image classification, object detection, segmentation, and video understanding.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-05-22T02:22:47+00:00', 'source': './Rag_data\\\\vision_transformer.pdf', 'file_path': './Rag_data\\\\vision_transformer.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-05-22T02:22:47+00:00', 'trapped': '', 'modDate': \"D:20250522022247+00'00'\", 'creationDate': \"D:20250522022247+00'00'\", 'page': 5}, page_content='Vision Transformers (ViT) adapt the Transformer model for computer vision tasks by converting images\\ninto sequences of patches.\\nEach patch is flattened and embedded before being passed to a standard Transformer encoder.\\nThis approach contrasts with CNNs, which process images using localized filters and hierarchical\\nrepresentations.\\nViTs have shown that, given sufficient data and compute, they can outperform traditional CNNs.\\nVariants like DeiT, Swin Transformer, and PiT have improved the efficiency and performance of ViTs.\\nApplications include image classification, object detection, segmentation, and video understanding.')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b7a1d6",
   "metadata": {},
   "source": [
    "Embedding and Vector Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee105613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Initialize embedding model\n",
    "openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "\n",
    "# Use only PDF chunks\n",
    "total_docs = paper_docs\n",
    "\n",
    "# Create and persist the vector DB\n",
    "chroma_db = Chroma.from_documents(\n",
    "    documents=total_docs,\n",
    "    collection_name='my_db',\n",
    "    embedding=openai_embed_model,\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "    persist_directory=\"./my_db\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ab7fcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_db = Chroma(persist_directory=\"./my_db\",\n",
    "collection_name='my_db',\n",
    "embedding_function=openai_embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "939b2e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: {'creationdate': '2025-05-22T02:22:47+00:00', 'keywords': '', 'file_path': './Rag_data\\\\transformer_paper.pdf', 'source': './Rag_data\\\\transformer_paper.pdf', 'creator': '(unspecified)', 'producer': 'ReportLab PDF Library - www.reportlab.com', 'format': 'PDF 1.4', 'subject': '(unspecified)', 'creationDate': \"D:20250522022247+00'00'\", 'total_pages': 6, 'author': '(anonymous)', 'modDate': \"D:20250522022247+00'00'\", 'trapped': '', 'page': 0, 'title': '(anonymous)', 'moddate': '2025-05-22T02:22:47+00:00'}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The Transformer architecture, introduced in 2017 by Vaswani et al., is built entirely on attention\n",
       "mechanisms, removing recurrence entirely.\n",
       "It consists of an encoder-decoder structure where both the encoder and decoder are made up of layers\n",
       "containing multi-head self-attention and feed-forward networks.\n",
       "Transformers leverage positional encoding to retain order information in sequences.\n",
       "The architecture allows for better parallelization and efficiency during training compared to RNNs.\n",
       "Transformers form the backbone of modern NLP models such as BERT, GPT, T5, and more.\n",
       "They have also been adapted for use in image processing, audio modeling, and multi-modal tasks.\n",
       "The Transformer architecture, introduced in 2017 by Vaswani et al., is built entirely on attention\n",
       "mechanisms, removing recurrence entirely.\n",
       "It consists of an encoder-decoder structure where both the encoder and decoder are made up of layers\n",
       "containing multi-head self-attention and feed-forward networks.\n",
       "Transformers leverage p"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'page': 3, 'source': './Rag_data\\\\transformer_paper.pdf', 'producer': 'ReportLab PDF Library - www.reportlab.com', 'keywords': '', 'moddate': '2025-05-22T02:22:47+00:00', 'creationdate': '2025-05-22T02:22:47+00:00', 'total_pages': 6, 'subject': '(unspecified)', 'format': 'PDF 1.4', 'creator': '(unspecified)', 'trapped': '', 'creationDate': \"D:20250522022247+00'00'\", 'title': '(anonymous)', 'modDate': \"D:20250522022247+00'00'\", 'author': '(anonymous)', 'file_path': './Rag_data\\\\transformer_paper.pdf'}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Transformers leverage positional encoding to retain order information in sequences.\n",
       "The architecture allows for better parallelization and efficiency during training compared to RNNs.\n",
       "Transformers form the backbone of modern NLP models such as BERT, GPT, T5, and more.\n",
       "They have also been adapted for use in image processing, audio modeling, and multi-modal tasks.\n",
       "The Transformer architecture, introduced in 2017 by Vaswani et al., is built entirely on attention\n",
       "mechanisms, removing recurrence entirely.\n",
       "It consists of an encoder-decoder structure where both the encoder and decoder are made up of layers\n",
       "containing multi-head self-attention and feed-forward networks.\n",
       "Transformers leverage positional encoding to retain order information in sequences.\n",
       "The architecture allows for better parallelization and efficiency during training compared to RNNs.\n",
       "Transformers form the backbone of modern NLP models such as BERT, GPT, T5, and more.\n",
       "They have also been adapted for use in image processing, aud"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'author': '(anonymous)', 'title': '(anonymous)', 'file_path': './Rag_data\\\\transformer_paper.pdf', 'creationDate': \"D:20250522022247+00'00'\", 'total_pages': 6, 'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'keywords': '', 'format': 'PDF 1.4', 'creationdate': '2025-05-22T02:22:47+00:00', 'moddate': '2025-05-22T02:22:47+00:00', 'source': './Rag_data\\\\transformer_paper.pdf', 'trapped': '', 'page': 5, 'subject': '(unspecified)', 'modDate': \"D:20250522022247+00'00'\"}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The Transformer architecture, introduced in 2017 by Vaswani et al., is built entirely on attention\n",
       "mechanisms, removing recurrence entirely.\n",
       "It consists of an encoder-decoder structure where both the encoder and decoder are made up of layers\n",
       "containing multi-head self-attention and feed-forward networks.\n",
       "Transformers leverage positional encoding to retain order information in sequences.\n",
       "The architecture allows for better parallelization and efficiency during training compared to RNNs.\n",
       "Transformers form the backbone of modern NLP models such as BERT, GPT, T5, and more.\n",
       "They have also been adapted for use in image processing, audio modeling, and multi-modal tasks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'modDate': \"D:20250522022247+00'00'\", 'subject': '(unspecified)', 'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'moddate': '2025-05-22T02:22:47+00:00', 'source': './Rag_data\\\\attention_paper.pdf', 'keywords': '', 'total_pages': 9, 'title': '(anonymous)', 'author': '(anonymous)', 'format': 'PDF 1.4', 'creationDate': \"D:20250522022247+00'00'\", 'trapped': '', 'page': 2, 'file_path': './Rag_data\\\\attention_paper.pdf', 'creationdate': '2025-05-22T02:22:47+00:00'}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "This allows the model to capture relationships regardless of distance in the input.\n",
       "Multi-head attention enables the model to jointly attend to information from different representation\n",
       "subspaces.\n",
       "The attention mechanism is a core part of architectures like BERT, GPT, and T5.\n",
       "It supports language understanding, generation, translation, and more.\n",
       "The attention mechanism allows neural networks to dynamically focus on different parts of the input\n",
       "sequence.\n",
       "It was first introduced in the context of machine translation with the goal of improving alignment\n",
       "between source and target languages.\n",
       "Attention mechanisms assign varying levels of importance to different words or tokens in a sequence,\n",
       "improving context comprehension.\n",
       "In practice, attention uses query, key, and value matrices to calculate weighted outputs.\n",
       "Self-attention, introduced in the Transformer model, enables each token to attend to all other tokens in\n",
       "the sequence.\n",
       "This allows the model to capture relationships regardless of di"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'moddate': '2025-05-22T02:22:47+00:00', 'keywords': '', 'creator': '(unspecified)', 'producer': 'ReportLab PDF Library - www.reportlab.com', 'creationDate': \"D:20250522022247+00'00'\", 'creationdate': '2025-05-22T02:22:47+00:00', 'format': 'PDF 1.4', 'source': './Rag_data\\\\attention_paper.pdf', 'file_path': './Rag_data\\\\attention_paper.pdf', 'total_pages': 9, 'subject': '(unspecified)', 'title': '(anonymous)', 'modDate': \"D:20250522022247+00'00'\", 'page': 3, 'trapped': '', 'author': '(anonymous)'}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "It supports language understanding, generation, translation, and more.\n",
       "The attention mechanism allows neural networks to dynamically focus on different parts of the input\n",
       "sequence.\n",
       "It was first introduced in the context of machine translation with the goal of improving alignment\n",
       "between source and target languages.\n",
       "Attention mechanisms assign varying levels of importance to different words or tokens in a sequence,\n",
       "improving context comprehension.\n",
       "In practice, attention uses query, key, and value matrices to calculate weighted outputs.\n",
       "Self-attention, introduced in the Transformer model, enables each token to attend to all other tokens in\n",
       "the sequence.\n",
       "This allows the model to capture relationships regardless of distance in the input.\n",
       "Multi-head attention enables the model to jointly attend to information from different representation\n",
       "subspaces.\n",
       "The attention mechanism is a core part of architectures like BERT, GPT, and T5.\n",
       "It supports language understanding, generation, translation, an"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Semantic Retrieval\n",
    "\n",
    "# Create a similarity retriever from the vector store\n",
    "similarity_retriever = chroma_db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5}\n",
    ")\n",
    "\n",
    "# Define your query\n",
    "query = \"What is machine learning?\"\n",
    "\n",
    "# Perform the search\n",
    "top_docs = similarity_retriever.invoke(query)\n",
    "\n",
    "# Display the results in a readable format\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def display_docs(docs):\n",
    "    for doc in docs:\n",
    "        print('Metadata:', doc.metadata)\n",
    "        print('Content Brief:')\n",
    "        display(Markdown(doc.page_content[:1000]))\n",
    "        print()\n",
    "\n",
    "# Call the display function\n",
    "display_docs(top_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaca2f7",
   "metadata": {},
   "source": [
    "RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3885b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define the RAG-style prompt\n",
    "rag_prompt = \"\"\"You are an assistant who is an expert in question-answering tasks.\n",
    "Answer the following question using only the following pieces of retrieved context.\n",
    "If the answer is not in the context, do not make up answers, just say that you don't know.\n",
    "Keep the answer detailed and well formatted based on the information from the context.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Create a ChatPromptTemplate\n",
    "rag_prompt_template = ChatPromptTemplate.from_template(rag_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67065abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the ChatGPT model\n",
    "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Function to format retrieved documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Construct the RAG pipeline\n",
    "qa_rag_chain = (\n",
    "    {\n",
    "        \"context\": similarity_retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | rag_prompt_template\n",
    "    | chatgpt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "105e6671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='A Vision Transformer (ViT) is a model that adapts the Transformer architecture for computer vision tasks by converting images into sequences of patches. Here are the key details about Vision Transformers:\\n\\n1. **Patch Processing**: In ViTs, images are divided into smaller patches, which are then flattened and embedded. This embedded representation is subsequently passed to a standard Transformer encoder.\\n\\n2. **Contrast with CNNs**: This method of processing images is different from Convolutional Neural Networks (CNNs), which utilize localized filters and hierarchical representations to analyze images.\\n\\n3. **Performance**: Vision Transformers have demonstrated the capability to outperform traditional CNNs, particularly when provided with sufficient data and computational resources.\\n\\n4. **Variants**: There are several variants of Vision Transformers, including DeiT (Data-efficient Image Transformers), Swin Transformer, and PiT (Pooling-based Image Transformers), which have been developed to enhance the efficiency and performance of the original ViT model.\\n\\n5. **Applications**: ViTs are applicable in various computer vision tasks, including:\\n   - Image classification\\n   - Object detection\\n   - Segmentation\\n   - Video understanding\\n\\nOverall, Vision Transformers represent a significant advancement in the field of computer vision, leveraging the strengths of the Transformer model to process visual data effectively.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 258, 'prompt_tokens': 1894, 'total_tokens': 2152, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_54eb4bd693', 'id': 'chatcmpl-BZq7wMcge7IU0qdfyabri0VaVW1hX', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--30669716-e29a-466e-b58d-baa0b50c10cc-0', usage_metadata={'input_tokens': 1894, 'output_tokens': 258, 'total_tokens': 2152, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_rag_chain.invoke(\"What is a Vision Transformer?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9161591e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I don't know."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"What is LangGraph?\"\n",
    "result = qa_rag_chain.invoke(query)\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97c44d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
